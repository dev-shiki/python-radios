#!/usr/bin/env python
"""
AI-powered test generator that works with any Python project structure.
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Set, Union
import ast
import re
import openai
import pytest


class UniversalTestGenerator:
    """Simple, universal test generator for any Python project."""
    
    def __init__(self, 
                 api_key: str,
                 coverage_threshold: float = 80.0,
                 model: str = "google/gemini-2.0-flash-001"):
        """Initialize with minimal configuration."""
        self.api_key = api_key
        self.coverage_threshold = coverage_threshold
        self.model = model
        self.openai_client = self._setup_openai()
        self.module_name = ""
        self.test_file_path = None
    
    def _setup_openai(self):
        """Configure OpenAI client for various providers."""
        # Support multiple providers by checking API key pattern
        if "openrouter" in self.api_key.lower() or os.getenv("OPENROUTER_API_KEY"):
            return openai.OpenAI(
                api_key=self.api_key,
                base_url="https://openrouter.ai/api/v1"
            )
        # Default to OpenAI
        return openai.OpenAI(api_key=self.api_key)
    
    def find_files_needing_tests(self, 
                               coverage_data: Dict = None,
                               target_files: List[str] = None) -> List[Path]:
        """
        Find Python files that need tests.
        
        Args:
            coverage_data: Coverage data from pytest-cov
            target_files: Specific files to target
        
        Returns:
            List of Python file paths that need tests
        """
        files_to_test = []
        
        # If specific files are requested
        if target_files:
            for file_path in target_files:
                path = Path(file_path)
                if path.exists() and path.suffix == '.py':
                    files_to_test.append(path)
            return files_to_test
        
        # Otherwise, find files based on coverage data
        if coverage_data:
            # Extract files with low coverage
            for file_path, data in coverage_data.get('files', {}).items():
                coverage_pct = data.get('summary', {}).get('percent_covered', 0)
                if coverage_pct < self.coverage_threshold:
                    files_to_test.append(Path(file_path))
        else:
            # Fallback: find all Python files
            files_to_test = list(Path('.').rglob('*.py'))
            # Exclude common directories
            files_to_test = [f for f in files_to_test 
                           if not any(part.startswith('.') or part == '__pycache__' 
                                    or 'test' in part for part in f.parts)]
        
        return files_to_test
    
    def generate_test_for_file(self, file_path: Path) -> str:
        """
        Generate tests for a single Python file.
        
        Args:
            file_path: Path to the Python file
            
        Returns:
            Generated test code
        """
        # Set module name and test file path
        self.module_name = self._get_module_name(file_path)
        self.test_file_path = self._get_test_path(file_path)
        
        # Read the source code
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                source_code = f.read()
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return ""
        
        # Create prompt with enhanced analysis
        prompt = self._create_prompt(file_path, source_code)
        
        # Generate tests
        try:
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._get_system_prompt()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1
            )
            
            # Get the response content
            generated_code = response.choices[0].message.content
            
            # Clean up the code (remove markdown formatting, etc.)
            clean_code = self._clean_generated_code(generated_code)
            
            return clean_code
        except Exception as e:
            print(f"Error generating tests for {file_path}: {e}")
            return ""
    
    def _clean_generated_code(self, code: str) -> str:
        """
        Clean generated code by removing Markdown artifacts and other non-Python syntax.
        
        Args:
            code: The code generated by the AI
            
        Returns:
            Cleaned Python code
        """
        # Remove markdown code block delimiters if present
        code = re.sub(r'```python\s*', '', code)
        code = re.sub(r'```\s*$', '', code, flags=re.MULTILINE)
        code = re.sub(r'```\s*', '', code)
        
        # Remove any leading/trailing whitespace
        code = code.strip()
        
        # Check if first non-empty line looks like an import statement or function/class definition
        # If not, we might have additional text before the actual code
        lines = code.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]
        
        if non_empty_lines and not (
            non_empty_lines[0].startswith('import ') or 
            non_empty_lines[0].startswith('from ') or
            non_empty_lines[0].startswith('def ') or
            non_empty_lines[0].startswith('class ') or
            non_empty_lines[0].startswith('#') or
            non_empty_lines[0].startswith('@')
        ):
            # Try to find the start of actual Python code
            for i, line in enumerate(non_empty_lines):
                if (line.startswith('import ') or 
                    line.startswith('from ') or 
                    line.startswith('def ') or
                    line.startswith('class ')):
                    # Found the start of code, remove everything before it
                    code = '\n'.join(lines[lines.index(line):])
                    break
        
        # Remove any "Output:" or similar text at the end
        code = re.sub(r'\nOutput:.*$', '', code, flags=re.DOTALL)
        
        return code
    
    def _get_system_prompt(self) -> str:
        """Return the system prompt for the AI model."""
        return """You are a master Python test generator. Your expertise:
- Write concise yet comprehensive pytest tests
- Mock external dependencies perfectly  
- Test edge cases and error paths
- Use correct async/await patterns
- Name tests clearly and descriptively

IMPORTANT: Do NOT include any Markdown formatting (like ```python) in your output. 
Return only clean Python code without backticks or any non-Python syntax.

Always deliver production-ready, minimal test code that achieves maximum coverage."""

    def _get_module_name(self, file_path: Path) -> str:
        """Convert file path to importable module name."""
        current_dir = Path.cwd()
        
        try:
            rel_path = file_path.relative_to(current_dir)
        except ValueError:
            rel_path = file_path
        
        parts = list(rel_path.parts)
        
        # Handle src directory
        if parts and parts[0] == "src":
            parts.pop(0)
        
        # Remove .py extension
        if parts and parts[-1].endswith('.py'):
            parts[-1] = parts[-1][:-3]  # Remove .py extension
        
        return ".".join(parts)

    def _get_function_args(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> List[Dict]:
        """Extract function arguments with their types and defaults."""
        args = []
        
        # Regular arguments
        for i, arg in enumerate(node.args.args):
            arg_info = {
                "name": arg.arg,
                "type": ast.unparse(arg.annotation) if arg.annotation else None,
                "default": None
            }
            
            # Check for default values
            defaults_start = len(node.args.args) - len(node.args.defaults)
            if i >= defaults_start:
                default_idx = i - defaults_start
                arg_info["default"] = ast.unparse(node.args.defaults[default_idx])
            
            args.append(arg_info)
        
        # *args
        if node.args.vararg:
            args.append({
                "name": f"*{node.args.vararg.arg}",
                "type": "*args",
                "default": None
            })
        
        # **kwargs
        if node.args.kwarg:
            args.append({
                "name": f"**{node.args.kwarg.arg}",
                "type": "**kwargs",
                "default": None
            })
        
        return args

    def _extract_return_type(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> Optional[str]:
        """Extract return type annotation from function definition."""
        if node.returns:
            return ast.unparse(node.returns)
        return None

    def _extract_uncovered_functions(self, source_code: str) -> Dict[str, Dict]:
        """Extract functions that need testing from source code."""
        uncovered_functions = {}
        
        try:
            tree = ast.parse(source_code)
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    func_name = node.name
                    
                    # Skip private functions (but keep dunder methods)
                    if func_name.startswith("_") and not (func_name.startswith("__") and func_name.endswith("__")):
                        continue
                    
                    uncovered_functions[func_name] = {
                        "args": self._get_function_args(node),
                        "is_async": isinstance(node, ast.AsyncFunctionDef),
                        "docstring": ast.get_docstring(node) or "",
                        "return_type": self._extract_return_type(node),
                        "line_no": node.lineno
                    }
                
                elif isinstance(node, ast.ClassDef):
                    class_name = node.name
                    for method in node.body:
                        if isinstance(method, (ast.FunctionDef, ast.AsyncFunctionDef)):
                            method_name = method.name
                            
                            # Skip private methods
                            if method_name.startswith("_") and not (method_name.startswith("__") and method_name.endswith("__")):
                                continue
                            
                            full_name = f"{class_name}.{method_name}"
                            uncovered_functions[full_name] = {
                                "args": self._get_function_args(method),
                                "is_async": isinstance(method, ast.AsyncFunctionDef),
                                "docstring": ast.get_docstring(method) or "",
                                "class_name": class_name,
                                "return_type": self._extract_return_type(method),
                                "line_no": method.lineno
                            }
        except Exception as e:
            print(f"Error extracting functions: {e}")
        
        return uncovered_functions

    def _identify_used_libraries(self, source_code: str) -> List[str]:
        """Identify external libraries used in the module."""
        libraries = set()
        
        # Standard library modules that shouldn't be considered as external
        stdlib_modules = {'os', 'sys', 'json', 're', 'pathlib', 'datetime', 'collections', 'typing', 
                        'functools', 'itertools', 'asyncio', 'logging', 'unittest', 'dataclasses'}
        
        try:
            tree = ast.parse(source_code)
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for name in node.names:
                        base_module = name.name.split('.')[0]
                        if base_module not in stdlib_modules:
                            libraries.add(base_module)
                
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        base_module = node.module.split('.')[0]
                        if base_module not in stdlib_modules:
                            libraries.add(base_module)
        except Exception as e:
            print(f"Error identifying libraries: {e}")
        
        return sorted(list(libraries))

    def _extract_detailed_function_info(self, uncovered_functions: Dict[str, Dict]) -> str:
        """Create detailed text description of functions needing tests."""
        details = []
        
        for func_name, info in uncovered_functions.items():
            desc = f"\nFunction: {func_name}"
            
            if info.get("is_async"):
                desc += " (async)"
            
            desc += f"\nLine: {info.get('line_no', 'unknown')}"
            
            # Arguments
            args = info.get("args", [])
            if args:
                desc += "\nArguments:"
                for arg in args:
                    arg_desc = f"  - {arg['name']}"
                    if arg.get('type'):
                        arg_desc += f": {arg['type']}"
                    if arg.get('default'):
                        arg_desc += f" = {arg['default']}"
                    desc += f"\n{arg_desc}"
            
            # Return type
            if info.get("return_type"):
                desc += f"\nReturns: {info['return_type']}"
            
            # Docstring
            if info.get("docstring"):
                desc += f"\nDocstring: {info['docstring']}"
            
            details.append(desc)
        
        return "\n".join(details)

    def find_model_references(self, source_file: Path) -> List[Path]:
        """
        Find model definition files referenced by the source file.
        
        Args:
            source_file: Path to the source file being tested
            
        Returns:
            List of paths to model definition files
        """
        model_files = set()
        source_dir = source_file.parent
        project_root = self._find_project_root(source_file)
        
        # Read the source file to extract imports
        try:
            with open(source_file, 'r', encoding='utf-8') as f:
                source_code = f.read()
        except Exception as e:
            print(f"Error reading {source_file}: {e}")
            return []
        
        # Extract imports from the source code
        imports = self._extract_imports(source_code)
        
        # Check common model file locations
        potential_paths = []
        
        # 1. Directly imported modules
        for module_path in imports:
            # Convert module path to file path
            parts = module_path.split('.')
            
            # Try absolute path from project root
            file_path = project_root.joinpath(*parts).with_suffix('.py')
            if file_path.exists():
                potential_paths.append(file_path)
            
            # Try relative path from source directory
            file_path = source_dir.joinpath(*parts).with_suffix('.py')
            if file_path.exists():
                potential_paths.append(file_path)
        
        # 2. Common model files in the same package
        common_model_files = ['models.py', 'schemas.py', 'entities.py', 'types.py', 'dataclasses.py']
        for model_file in common_model_files:
            file_path = source_dir / model_file
            if file_path.exists():
                potential_paths.append(file_path)
        
        # 3. Check if there's a models directory
        model_dirs = [source_dir / 'models', project_root / 'models', 
                      source_dir / 'schemas', project_root / 'schemas']
        
        for model_dir in model_dirs:
            if model_dir.exists() and model_dir.is_dir():
                for file_path in model_dir.glob('**/*.py'):
                    potential_paths.append(file_path)
        
        # Analyze each potential file to check if it contains model definitions
        for file_path in potential_paths:
            if self._contains_model_definitions(file_path):
                model_files.add(file_path)
        
        return list(model_files)

    def _find_project_root(self, file_path: Path) -> Path:
        """Find the project root directory."""
        current_path = file_path.parent.absolute()
        
        # Look for common project root indicators
        indicators = ['pyproject.toml', 'setup.py', '.git', 'requirements.txt']
        
        while current_path != current_path.parent:
            for indicator in indicators:
                if (current_path / indicator).exists():
                    return current_path
            current_path = current_path.parent
        
        # If no root found, return the current directory
        return Path.cwd()

    def _extract_imports(self, source_code: str) -> List[str]:
        """Extract imported modules from source code."""
        imports = []
        
        try:
            tree = ast.parse(source_code)
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for name in node.names:
                        imports.append(name.name)
                
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.append(node.module)
        except Exception as e:
            print(f"Error extracting imports: {e}")
        
        return imports

    def _contains_model_definitions(self, file_path: Path) -> bool:
        """
        Check if a file contains model class definitions (dataclasses, mashumaro, pydantic, etc.).
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Quick check for common model indicators
            indicators = [
                'class', '@dataclass', 'mashumaro', 'BaseModel', 
                'DataClassJSONMixin', 'SerializationMixin', 'field'
            ]
            
            if not any(indicator in content for indicator in indicators):
                return False
            
            # More detailed AST analysis
            tree = ast.parse(content)
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    # Check decorators
                    for decorator in node.decorator_list:
                        decorator_str = ast.unparse(decorator)
                        if any(model_dec in decorator_str for model_dec in ['dataclass', 'BaseModel']):
                            return True
                    
                    # Check base classes
                    for base in node.bases:
                        base_str = ast.unparse(base)
                        if any(model_base in base_str for model_base in 
                              ['DataClassJSONMixin', 'BaseModel', 'SerializationMixin']):
                            return True
                    
                    # Check for model-like structure (many typed attributes)
                    typed_attrs = sum(1 for item in node.body 
                                    if isinstance(item, ast.AnnAssign) and isinstance(item.target, ast.Name))
                    if typed_attrs >= 3:  # If class has several typed attributes, it's likely a model
                        return True
            
            return False
        except Exception as e:
            print(f"Error analyzing {file_path}: {e}")
            return False

    def extract_model_definitions(self, file_path: Path) -> str:
        """
        Extract model class definitions from a file.
        
        Args:
            file_path: Path to the file
            
        Returns:
            String containing model class definitions
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            model_definitions = []
            
            # Get imports that might be needed for the models
            imports = []
            for node in ast.walk(tree):
                if isinstance(node, (ast.Import, ast.ImportFrom)):
                    imports.append(ast.unparse(node))
            
            # Extract class definitions that appear to be models
            for node in ast.iter_child_nodes(tree):
                if isinstance(node, ast.ClassDef):
                    # Check if it's a model class
                    is_model = False
                    
                    # Check decorators
                    for decorator in node.decorator_list:
                        decorator_str = ast.unparse(decorator)
                        if any(model_dec in decorator_str for model_dec in ['dataclass', 'BaseModel']):
                            is_model = True
                            break
                    
                    # Check base classes
                    if not is_model:
                        for base in node.bases:
                            base_str = ast.unparse(base)
                            if any(model_base in base_str for model_base in 
                                  ['DataClassJSONMixin', 'BaseModel', 'SerializationMixin']):
                                is_model = True
                                break
                    
                    # Check for model-like structure (many typed attributes)
                    if not is_model:
                        typed_attrs = sum(1 for item in node.body 
                                        if isinstance(item, ast.AnnAssign) and isinstance(item.target, ast.Name))
                        if typed_attrs >= 3:  # If class has several typed attributes, it's likely a model
                            is_model = True
                    
                    if is_model:
                        model_definitions.append(ast.unparse(node))
            
            # Combine imports and model definitions
            unique_imports = list(dict.fromkeys(imports))  # Remove duplicates while preserving order
            model_code = "\n".join(unique_imports + [""] + model_definitions)
            
            return model_code
        except Exception as e:
            print(f"Error extracting model definitions from {file_path}: {e}")
            return ""

    def _create_prompt(self, source_file: Path, source_code: str) -> str:
        """Create a prompt for generating tests with model reference files."""
        # Extract information
        uncovered_functions = self._extract_uncovered_functions(source_code)
        used_libraries = self._identify_used_libraries(source_code)
        
        # Find and extract model reference files
        model_reference_files = self.find_model_references(source_file)
        model_reference_code = ""
        
        if model_reference_files:
            model_reference_code = "\nMODEL REFERENCE FILES:\n"
            for file_path in model_reference_files:
                model_definitions = self.extract_model_definitions(file_path)
                if model_definitions:
                    # Use str(file_path) instead of trying to make it relative
                    # This avoids the ValueError when paths can't be made relative
                    model_reference_code += f"\n# From {file_path}\n```python\n{model_definitions}\n```\n"
        
        # Extract function details
        function_details = self._extract_detailed_function_info(uncovered_functions)
        
        prompt = f"""You are an expert Python testing specialist. Generate comprehensive pytest test cases for the given code.

MODULE INFORMATION:
- File: {source_file}
- Module path: {self.module_name}
- Test file path: {self.test_file_path}

SOURCE CODE:
```python
{source_code}
```

{model_reference_code}

FUNCTIONS REQUIRING TESTS:
{function_details}

LIBRARY CONTEXT:
- Used libraries: {', '.join(used_libraries)}

TEST GENERATION REQUIREMENTS:
## STRICT GUIDELINES
1. Write ONLY valid Python test code with no explanations or markdown
2. Include proper imports for ALL required packages and modules
3. Import the module under test correctly
4. Focus on COMPLETE test coverage for functions with low coverage
5. For model classes and mock responses:
   - Analyze model structures carefully to include ALL required fields
   - Pay close attention to field case sensitivity (stationcount vs STATIONCOUNT)
   - Ensure common fields like 'supported_version', 'code', and 'bitrate' are included
   - Match field types exactly with what models expect (int, Optional[str], etc.)
   - When in doubt, include more fields rather than fewer
6. For asynchronous code:
   - Use appropriate async patterns throughout
   - Handle coroutines correctly in all contexts
   - NEVER iterate directly over coroutines (to avoid "TypeError: argument of type 'coroutine' is not iterable")
   - Always properly await every coroutine before using its result
   - Use AsyncMock for mocking any async functions
   - When mocking async functions, ensure return values are awaitable with AsyncMock(return_value=value)
   - For testing async code that returns lists or iterables, ensure you await the coroutine before iterating
7. For mocking external services:
   - Configure mocks to handle the exact number of expected calls
   - Set up appropriate return values and side effects
   - Ensure proper resource cleanup by verifying necessary method calls on closable resources
   - For context managers or resources that need cleanup, assert all cleanup methods are called
   - Pay attention to connection objects that should be properly closed after use
8. For API endpoints:
   - Use EXACT endpoint paths from the source code
   - Check each path segment carefully (e.g., '/click/' might be required in a path)
   - Verify URL structures match exactly between tests and API implementation
   - Compare test assertions with actual API calls to ensure path alignment
   - Pay attention to endpoint variations like 'url/click/uuid' vs 'url/uuid'
9. For error handling:
   - Test all relevant error scenarios
   - Import and use correct exception classes
10. Create descriptive test function names that indicate what is being tested
11. IMPORTANT: DO NOT use pytest-mock fixtures (mocker). Use unittest.mock directly:
    ```python
    from unittest.mock import patch, AsyncMock, MagicMock, call
    ```
12. Use class-level fixtures with self parameter instead of function-level fixtures when testing classes

CRITICAL AREAS TO ANALYZE:
Examine the source code and model references carefully to identify:
- Required model fields and their exact types
- Case sensitivity in field names (especially STATIONCOUNT vs stationcount)
- API endpoint URL patterns and formats
- Asynchronous function behavior
- External service interaction patterns
- Error handling approaches

COMMON ASYNC TESTING ISSUES TO AVOID:
- TypeError: argument of type 'coroutine' is not iterable - Always await coroutines before iterating
- TypeError: object MagicMock can't be used in 'await' expression - Use AsyncMock instead
- AssertionError: Expected 'query' to be called once. Called 5 times - Configure mock side_effects properly

RESULT FORMAT (just the code, no explanations):
```python
# Complete test file with imports, fixtures, and test functions
```
"""
    
        return prompt
    
    def save_test_file(self, file_path: Path, test_code: str) -> Path:
        """
        Save the generated test file.
        
        Args:
            file_path: Original Python file
            test_code: Generated test code
            
        Returns:
            Path to the saved test file
        """
        # Determine test file path if not already set
        if not self.test_file_path:
            self.test_file_path = self._get_test_path(file_path)
        
        # Create directories if needed
        self.test_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Save the test file
        try:
            with open(self.test_file_path, 'w', encoding='utf-8') as f:
                f.write(test_code)
            print(f"Saved test file: {self.test_file_path}")
            return self.test_file_path
        except Exception as e:
            print(f"Error saving test file: {e}")
            return None
    
    def _get_test_path(self, source_file: Path) -> Path:
        """Determine where to save the test file."""
        # Try to find tests directory
        current_dir = Path.cwd()
        tests_dir = current_dir / 'tests'
        
        if not tests_dir.exists():
            tests_dir = current_dir / 'test'
        
        if not tests_dir.exists():
            tests_dir = current_dir
        
        # Create test filename
        test_filename = f"test_{source_file.stem}.py"
        
        # Try to match directory structure
        try:
            rel_path = source_file.relative_to(current_dir)
            if len(rel_path.parts) > 1:
                # Create directory structure in tests
                subdir = tests_dir / Path(*rel_path.parts[:-1])
                subdir.mkdir(parents=True, exist_ok=True)
                return subdir / test_filename
        except ValueError:
            pass
        
        return tests_dir / test_filename


def main():
    """Main entry point."""
    # Get configuration from environment
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        print("Error: No API key found. Set OPENROUTER_API_KEY or OPENAI_API_KEY")
        sys.exit(1)
    
    # Get optional model
    model = os.getenv("OPENAI_MODEL", "google/gemini-2.0-flash-001")
    
    coverage_threshold = float(os.getenv("COVERAGE_THRESHOLD", "80"))
    target_files_str = os.getenv("TARGET_FILES", "")
    target_files = [f.strip() for f in target_files_str.split(",") if f.strip()]
    
    # Initialize generator
    generator = UniversalTestGenerator(api_key, coverage_threshold, model)
    
    # Load coverage data if available
    coverage_data = None
    if Path("coverage-initial.json").exists():
        try:
            with open("coverage-initial.json", "r") as f:
                coverage_data = json.load(f)
        except Exception as e:
            print(f"Error loading coverage data: {e}")
    
    # Find files to test
    files_to_test = generator.find_files_needing_tests(coverage_data, target_files)
    
    if not files_to_test:
        print("No files found to generate tests for")
        return
    
    print(f"Generating tests for {len(files_to_test)} files...")
    
    # Generate tests for each file
    for file_path in files_to_test:
        print(f"Processing: {file_path}")
        test_code = generator.generate_test_for_file(file_path)
        
        if test_code:
            # Validate if the code is syntactically correct Python
            try:
                ast.parse(test_code)
                print(f"✅ Generated code is syntactically correct")
            except SyntaxError as e:
                print(f"⚠️ Warning: Generated code has syntax error: {e}")
                print("Attempting additional cleanup...")
                # Try more aggressive cleanup if normal cleanup didn't work
                test_code = test_code.replace('```', '')
                try:
                    ast.parse(test_code)
                    print(f"✅ Fixed syntax issues")
                except SyntaxError as e:
                    print(f"❌ Could not fix all syntax issues: {e}")
                    # Continue anyway, but warn the user
            
            # Save the test file
            generator.save_test_file(file_path, test_code)
    
    print("Test generation complete!")


if __name__ == "__main__":
    main()