name: Generate Tests

on:
  push:
  workflow_dispatch:
    inputs:
      coverage_threshold:
        description: 'Minimum coverage percentage to target'
        required: false
        default: '80'
      max_modules:
        description: 'Maximum number of modules to analyze (0 = no limit)'
        required: false
        default: '5'

env:
  DEFAULT_PYTHON: "3.11"
  OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
  COVERAGE_INITIAL: coverage-initial.json
  COVERAGE_FINAL: coverage-final.json

jobs:
  generate-tests:
    name: Generate Tests
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: ⤵️ Check out code from GitHub
        uses: actions/checkout@v4.2.2
        with:
          ref: revert-2-ai-generated-tests-14921708095  # Checkout branch ini

      - name: 🔐 Validate prerequisites
        run: |
          echo "🔍 Checking prerequisites..."
          
          # Create temp directory for this run
          RUN_ID="${GITHUB_RUN_ID}_${GITHUB_RUN_NUMBER}"
          export TEMP_DIR="/tmp/test_gen_${RUN_ID}"
          mkdir -p "${TEMP_DIR}"
          echo "TEMP_DIR=${TEMP_DIR}" >> $GITHUB_ENV
          
          # Check API key
          if [ -z "$OPENROUTER_API_KEY" ]; then
            echo "::error::OPENROUTER_API_KEY not set in secrets"
            exit 1
          fi
          echo "✅ API key found"
          
          # Get input parameters with defaults
          THRESHOLD="${{ github.event.inputs.coverage_threshold }}"
          MAX_MODULES="${{ github.event.inputs.max_modules }}"
          
          # Default values if empty
          THRESHOLD="${THRESHOLD:-80}"
          MAX_MODULES="${MAX_MODULES:-5}"
          
          # Clean inputs - remove all non-numeric characters
          THRESHOLD=$(echo "$THRESHOLD" | sed 's/[^0-9]//g')
          MAX_MODULES=$(echo "$MAX_MODULES" | sed 's/[^0-9]//g')
          
          # Set defaults if empty after cleaning
          THRESHOLD="${THRESHOLD:-80}"
          MAX_MODULES="${MAX_MODULES:-5}"
          
          # Validate coverage threshold (0-100)
          if [ "$THRESHOLD" -lt 0 ] 2>/dev/null || [ "$THRESHOLD" -gt 100 ] 2>/dev/null; then
            echo "::error::Invalid coverage_threshold: must be 0-100 (got $THRESHOLD)"
            exit 1
          fi
          
          # Validate max modules (>= 0)
          if [ "$MAX_MODULES" -lt 0 ] 2>/dev/null; then
            echo "::error::Invalid max_modules: must be >= 0 (got $MAX_MODULES)"
            exit 1
          fi
          
          # Export validated values
          echo "VALIDATED_THRESHOLD=$THRESHOLD" >> $GITHUB_ENV
          echo "VALIDATED_MAX_MODULES=$MAX_MODULES" >> $GITHUB_ENV
          
          echo "✅ Validated inputs:"
          echo "  Coverage threshold: $THRESHOLD%"
          echo "  Max modules: $MAX_MODULES"
          
          # Check source directory
          if [ ! -d "src" ] && [ ! -f "setup.py" ] && [ ! -f "pyproject.toml" ]; then
            echo "::warning::No standard Python project structure found"
          fi
          
          # Create necessary directories
          mkdir -p tests coverage-final .github/scripts
          
          # Initialize log files with headers
          echo '[]' > api_interaction_logs.json
          echo "# Test Generation Experiment Log" > generation_metrics.txt
          echo "Run ID: ${RUN_ID}" >> generation_metrics.txt
          echo "Started: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> generation_metrics.txt
          echo "Coverage Threshold: $THRESHOLD%" >> generation_metrics.txt
          echo "Max Modules: $MAX_MODULES" >> generation_metrics.txt
          echo "---" >> generation_metrics.txt
          
          echo "✅ Prerequisites validated"

      - name: 🏗 Set up Poetry
        run: |
          pipx install poetry
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true

      - name: 🏗 Set up Python ${{ env.DEFAULT_PYTHON }}
        uses: actions/setup-python@v5.5.0
        with:
          python-version: ${{ env.DEFAULT_PYTHON }}
          cache: "poetry"

      - name: 🏗 Install dependencies
        run: |
          set -euo pipefail
          
          # Install base dependencies
          poetry install --no-interaction || {
            echo "::error::Failed to install base dependencies"
            exit 1
          }
          
          # Ensure required packages are installed
          REQUIRED_PACKAGES="pytest pytest-cov pytest-asyncio openai"
          for package in $REQUIRED_PACKAGES; do
            if ! poetry show "$package" &>/dev/null; then
              echo "📦 Installing $package..."
              poetry add "$package" --group dev || {
                echo "::error::Failed to install $package"
                exit 1
              }
            fi
          done
          
          # Verify environment
          poetry check || {
            echo "::warning::Poetry environment check failed"
          }
          
          echo "✅ Dependencies ready"

      - name: 📊 Generate initial coverage
        id: initial-coverage
        run: |
          set -euo pipefail
          
          echo "📊 Generating initial coverage..."
          
          # Setup test directory
          touch tests/__init__.py
          
          # Handle no tests case
          if [ -z "$(find tests -name 'test_*.py' -not -name 'test_placeholder.py' 2>/dev/null || true)" ]; then
            echo "def test_placeholder(): pass" > tests/test_placeholder.py
            echo "::warning::No tests found, using placeholder"
          fi
          
          # Determine source directory
          if [ -d "src" ]; then
            SRC_DIR="src"
          else
            SRC_DIR="."
          fi
          echo "SRC_DIR=$SRC_DIR" >> $GITHUB_ENV
          
          # Generate coverage with better error handling
          poetry run pytest \
            --cov="$SRC_DIR" \
            --cov-report="json:${COVERAGE_INITIAL}" \
            --quiet \
            2>/dev/null || true
          
          # Ensure valid coverage file
          if [ ! -f "$COVERAGE_INITIAL" ] || [ ! -s "$COVERAGE_INITIAL" ]; then
            echo '{"totals": {"percent_covered": 0}, "files": {}}' > "$COVERAGE_INITIAL"
          fi
          
          # Extract coverage safely and trim newlines
          INITIAL_COV=$(jq -r '.totals.percent_covered // 0' "$COVERAGE_INITIAL")
          INITIAL_COV=$(echo "$INITIAL_COV" | tr -d '\n' | xargs)
          echo "initial_coverage=$INITIAL_COV" >> $GITHUB_OUTPUT
          echo "✅ Initial coverage: ${INITIAL_COV}%" | tee -a generation_metrics.txt

      - name: 🔍 Find modules needing tests
        id: find-modules
        run: |
          set -euo pipefail
          
          THRESHOLD=${{ env.VALIDATED_THRESHOLD }}
          echo "🔍 Finding modules below ${THRESHOLD}% coverage..."
          
          # Run gap finder
          if [ -f ".github/scripts/find_coverage_gaps.py" ]; then
            poetry run python .github/scripts/find_coverage_gaps.py "$THRESHOLD" > modules.json || {
              echo "::warning::Gap finder failed, using empty list"
              echo "[]" > modules.json
            }
          else
            echo "::error::find_coverage_gaps.py not found"
            echo "[]" > modules.json
          fi
          
          # Validate JSON
          if ! jq empty modules.json 2>/dev/null; then
            echo "[]" > modules.json
          fi
          
          # Count and output with trimmed newlines
          MODULE_COUNT=$(jq '. | length' modules.json)
          MODULE_COUNT=$(echo "$MODULE_COUNT" | tr -d '\n' | xargs)
          echo "module_count=$MODULE_COUNT" >> $GITHUB_OUTPUT
          echo "modules=$(cat modules.json | tr -d '\n')" >> $GITHUB_OUTPUT
          echo "✅ Found $MODULE_COUNT modules" | tee -a generation_metrics.txt
          
          # Log module list
          if [ "$MODULE_COUNT" -gt 0 ]; then
            echo "Modules:" | tee -a generation_metrics.txt
            jq -r '.[]' modules.json | sed 's/^/  - /' | tee -a generation_metrics.txt
          fi

      - name: 🤖 Generate tests
        id: generate
        run: |
          set -euo pipefail
          
          MODULE_COUNT=${{ steps.find-modules.outputs.module_count }}
          
          if [ "$MODULE_COUNT" -eq "0" ]; then
            echo "ℹ️ No modules need tests"
            echo "status=no_modules" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Verify script exists
          if [ ! -f ".github/scripts/generate_test2.py" ]; then
            echo "::error::generate_test2.py not found"
            echo "status=script_missing" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Process modules
          MODULES='${{ steps.find-modules.outputs.modules }}'
          MAX_MODULES=${{ env.VALIDATED_MAX_MODULES }}
          
          # Apply limit
          if [ "$MAX_MODULES" -gt 0 ] && [ "$MODULE_COUNT" -gt "$MAX_MODULES" ]; then
            MODULES=$(echo "$MODULES" | jq ".[:$MAX_MODULES]")
            echo "ℹ️ Limited to $MAX_MODULES modules" | tee -a generation_metrics.txt
          fi
          
          # Initialize counters
          echo "0" > "${TEMP_DIR}/success_count"
          echo "0" > "${TEMP_DIR}/failed_count"
          START_TIME=$(date +%s)
          
          # Process each module
          echo "$MODULES" | jq -r '.[]' | while IFS= read -r MODULE; do
            echo "🔄 Processing: $MODULE" | tee -a generation_metrics.txt
            
            if poetry run python .github/scripts/generate_test2.py --module "$MODULE"; then
              SUCCESS=$(($(cat "${TEMP_DIR}/success_count") + 1))
              echo "$SUCCESS" > "${TEMP_DIR}/success_count"
              echo "  ✅ Success" | tee -a generation_metrics.txt
            else
              FAILED=$(($(cat "${TEMP_DIR}/failed_count") + 1))
              echo "$FAILED" > "${TEMP_DIR}/failed_count"
              echo "  ❌ Failed" | tee -a generation_metrics.txt
            fi
          done
          
          # Read final counts and trim newlines
          SUCCESS=$(cat "${TEMP_DIR}/success_count" 2>/dev/null || echo "0")
          SUCCESS=$(echo "$SUCCESS" | tr -d '\n' | xargs)
          FAILED=$(cat "${TEMP_DIR}/failed_count" 2>/dev/null || echo "0")
          FAILED=$(echo "$FAILED" | tr -d '\n' | xargs)
          DURATION=$(($(date +%s) - START_TIME))
          
          echo "Generation Summary:" | tee -a generation_metrics.txt
          echo "  Duration: ${DURATION}s" | tee -a generation_metrics.txt
          echo "  Success: $SUCCESS" | tee -a generation_metrics.txt
          echo "  Failed: $FAILED" | tee -a generation_metrics.txt
          
          # Count generated tests
          TEST_COUNT=$(find tests -name "test_*.py" -not -name "test_placeholder.py" | wc -l)
          
          if [ "$TEST_COUNT" -gt 0 ]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "test_count=$(echo $TEST_COUNT | tr -d '\n')" >> $GITHUB_OUTPUT
            echo "✅ Generated $TEST_COUNT test files" | tee -a generation_metrics.txt
          else
            echo "status=no_tests" >> $GITHUB_OUTPUT
            echo "test_count=0" >> $GITHUB_OUTPUT
          fi

      - name: 🧪 Run tests
        id: test-run
        run: |
          set -euo pipefail
          
          # Count actual test files
          TEST_COUNT=$(find tests -name "test_*.py" -not -name "test_placeholder.py" | wc -l)
          
          if [ "$TEST_COUNT" -eq 0 ]; then
            echo "ℹ️ No tests to run"
            echo "status=no_tests" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "🧪 Running $TEST_COUNT test files" | tee -a generation_metrics.txt
          
          # Run tests (allow failure)
          set +e
          poetry run pytest tests/ \
            -v \
            --tb=short \
            --cov="${SRC_DIR}" \
            --cov-report="json:${COVERAGE_AFTER_GEN}" \
            2>&1 | tee test-results.log
          
          TEST_EXIT_CODE=$?
          set -e
          
          # Generate final reports
          poetry run pytest tests/ \
            --cov="${SRC_DIR}" \
            --cov-report="html:coverage-final/" \
            --cov-report="json:coverage-final/${COVERAGE_FINAL}" \
            --quiet 2>/dev/null || true
          
          # Analyze results with safe grep and trim newlines
          if [ "$TEST_EXIT_CODE" -ne 0 ]; then
            FAILURE_COUNT=$(grep -c "FAILED" test-results.log 2>/dev/null || echo "0")
            # Trim any whitespace/newlines
            FAILURE_COUNT=$(echo "$FAILURE_COUNT" | tr -d '\n' | xargs)
            echo "⚠️ $FAILURE_COUNT test failures" | tee -a generation_metrics.txt
            
            # Extract error details
            grep -A 5 "FAILED\|ERROR" test-results.log > test-errors.log 2>/dev/null || true
            
            echo "status=has_failures" >> $GITHUB_OUTPUT
            echo "failure_count=$(echo $FAILURE_COUNT | tr -d '\n')" >> $GITHUB_OUTPUT
          else
            echo "✅ All tests passed!" | tee -a generation_metrics.txt
            echo "status=all_passed" >> $GITHUB_OUTPUT
            echo "failure_count=0" >> $GITHUB_OUTPUT
          fi

      - name: 💾 Upload test files as artifact
        uses: actions/upload-artifact@v4.6.2
        with:
          name: test-files-${{ github.run_id }}
          path: tests/
          if-no-files-found: error

      - name: 💾 Upload coverage files as artifact
        uses: actions/upload-artifact@v4.6.2
        with:
          name: coverage-files-${{ github.run_id }}
          path: |
            coverage-initial.json
            coverage-final.json
          if-no-files-found: error

      - name: 📝 Create workflow summary
        if: always()
        run: |
          cat <<EOF >> $GITHUB_STEP_SUMMARY
          # Test Generation Summary
          
          ## Overview
          - **Initial Coverage**: ${{ steps.initial-coverage.outputs.initial_coverage }}%
          - **Modules Found**: ${{ steps.find-modules.outputs.module_count }}
          - **Tests Generated**: ${{ steps.generate.outputs.test_count }}
          - **Test Status**: ${{ steps.test-run.outputs.status }}
          
          ## Configuration
          - **Coverage Threshold**: ${{ env.VALIDATED_THRESHOLD }}%
          - **Max Modules**: ${{ env.VALIDATED_MAX_MODULES }}
          
          ## Details
          EOF
          
          if [ "${{ steps.test-run.outputs.status }}" = "has_failures" ]; then
            cat <<EOF >> $GITHUB_STEP_SUMMARY
          - **Initial Failures**: ${{ steps.test-run.outputs.failure_count }}
          EOF
          fi
          
          # Add coverage summary
          if [ -f "COVERAGE_SUMMARY.md" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            cat COVERAGE_SUMMARY.md >> $GITHUB_STEP_SUMMARY
          fi
          
          # Add metrics
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Metrics" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          tail -30 generation_metrics.txt >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No metrics"
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: �� Cleanup
        if: always()
        run: |
          # Remove temporary files
          rm -rf "${TEMP_DIR}" 2>/dev/null || true
          
          # Remove placeholder test if real tests exist
          if [ -f "tests/test_placeholder.py" ]; then
            REAL_TESTS=$(find tests -name "test_*.py" -not -name "test_placeholder.py" | wc -l)
            if [ "$REAL_TESTS" -gt 0 ]; then
              rm -f tests/test_placeholder.py
            fi
          fi