name: Generate Tests

on:
  push:
  workflow_dispatch:
    inputs:
      coverage_threshold:
        description: 'Persentase coverage minimum yang ditargetkan'
        required: false
        default: '80'
      max_modules:
        description: 'Jumlah maksimum modul yang dianalisis (0 = tidak terbatas)'
        required: false
        default: '5'

env:
  DEFAULT_PYTHON: "3.11"
  OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
  # Standardized coverage file names
  COVERAGE_INITIAL: coverage-initial.json
  COVERAGE_FINAL: coverage-final.json
  COVERAGE_AFTER_GEN: coverage-after-generation.json

jobs:
  generate-tests:
    name: Generate Tests
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: ⤵️ Check out code from GitHub
        uses: actions/checkout@v4.2.2

      - name: 🔐 Validate prerequisites
        run: |
          echo "🔍 Checking prerequisites..."
          
          # Membuat direktori temp untuk run ini
          RUN_ID="${GITHUB_RUN_ID}_${GITHUB_RUN_NUMBER}"
          export TEMP_DIR="/tmp/test_gen_${RUN_ID}"
          mkdir -p "${TEMP_DIR}"
          echo "TEMP_DIR=${TEMP_DIR}" >> $GITHUB_ENV
          
          # Cek API key
          if [ -z "$OPENROUTER_API_KEY" ]; then
            echo "::error::OPENROUTER_API_KEY tidak diset di secrets"
            exit 1
          fi
          echo "✅ API key ditemukan"
          
          # Ambil parameter input dengan default
          THRESHOLD="${{ github.event.inputs.coverage_threshold }}"
          MAX_MODULES="${{ github.event.inputs.max_modules }}"
          
          # Nilai default jika kosong
          THRESHOLD="${THRESHOLD:-80}"
          MAX_MODULES="${MAX_MODULES:-5}"
          
          # Bersihkan input - hapus semua karakter non-numerik
          THRESHOLD=$(echo "$THRESHOLD" | sed 's/[^0-9]//g')
          MAX_MODULES=$(echo "$MAX_MODULES" | sed 's/[^0-9]//g')
          
          # Set default jika kosong setelah pembersihan
          THRESHOLD="${THRESHOLD:-80}"
          MAX_MODULES="${MAX_MODULES:-5}"
          
          # Validasi threshold coverage (0-100)
          if [ "$THRESHOLD" -lt 0 ] 2>/dev/null || [ "$THRESHOLD" -gt 100 ] 2>/dev/null; then
            echo "::error::coverage_threshold tidak valid: harus 0-100 (dapat $THRESHOLD)"
            exit 1
          fi
          
          # Validasi max modules (>= 0)
          if [ "$MAX_MODULES" -lt 0 ] 2>/dev/null; then
            echo "::error::max_modules tidak valid: harus >= 0 (dapat $MAX_MODULES)"
            exit 1
          fi
          
          # Export nilai yang sudah divalidasi
          echo "VALIDATED_THRESHOLD=$THRESHOLD" >> $GITHUB_ENV
          echo "VALIDATED_MAX_MODULES=$MAX_MODULES" >> $GITHUB_ENV
          
          echo "✅ Input yang divalidasi:"
          echo "  Threshold coverage: $THRESHOLD%"
          echo "  Max modules: $MAX_MODULES"
          
          # Cek direktori source
          if [ ! -d "src" ] && [ ! -f "setup.py" ] && [ ! -f "pyproject.toml" ]; then
            echo "::warning::Struktur project Python standar tidak ditemukan"
          fi
          
          # Buat direktori yang diperlukan
          mkdir -p tests coverage-final .github/scripts
          
          # Inisialisasi file log dengan header
          echo '[]' > api_interaction_logs.json
          echo "# Log Eksperimen Generasi Test" > generation_metrics.txt
          echo "Run ID: ${RUN_ID}" >> generation_metrics.txt
          echo "Dimulai: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> generation_metrics.txt
          echo "Threshold Coverage: $THRESHOLD%" >> generation_metrics.txt
          echo "Max Modules: $MAX_MODULES" >> generation_metrics.txt
          echo "---" >> generation_metrics.txt
          
          echo "✅ Prerequisites divalidasi"

      - name: 🏗 Set up Poetry
        run: |
          pipx install poetry
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true

      - name: 🏗 Set up Python ${{ env.DEFAULT_PYTHON }}
        uses: actions/setup-python@v5.5.0
        with:
          python-version: ${{ env.DEFAULT_PYTHON }}
          cache: "poetry"

      - name: 🏗 Install dependencies
        run: |
          set -euo pipefail
          
          # Install base dependencies
          poetry install --no-interaction || {
            echo "::error::Failed to install base dependencies"
            exit 1
          }
          
          # Ensure required packages are installed
          REQUIRED_PACKAGES="pytest pytest-cov pytest-asyncio openai"
          for package in $REQUIRED_PACKAGES; do
            if ! poetry show "$package" &>/dev/null; then
              echo "📦 Installing $package..."
              poetry add "$package" --group dev || {
                echo "::error::Failed to install $package"
                exit 1
              }
            fi
          done
          
          # Verify environment
          poetry check || {
            echo "::warning::Poetry environment check failed"
          }
          
          echo "✅ Dependencies ready"

      - name: 📊 Generate initial coverage
        id: initial-coverage
        run: |
          set -euo pipefail
          
          echo "📊 Generating initial coverage..."
          
          # Setup test directory
          touch tests/__init__.py
          
          # Handle no tests case
          if [ -z "$(find tests -name 'test_*.py' -not -name 'test_placeholder.py' 2>/dev/null || true)" ]; then
            echo "def test_placeholder(): pass" > tests/test_placeholder.py
            echo "::warning::No tests found, using placeholder"
          fi
          
          # Determine source directory
          if [ -d "src" ]; then
            SRC_DIR="src"
          else
            SRC_DIR="."
          fi
          echo "SRC_DIR=$SRC_DIR" >> $GITHUB_ENV
          
          # Generate coverage with better error handling
          poetry run pytest \
            --cov="$SRC_DIR" \
            --cov-report="json:${COVERAGE_INITIAL}" \
            --quiet \
            2>/dev/null || true
          
          # Ensure valid coverage file
          if [ ! -f "$COVERAGE_INITIAL" ] || [ ! -s "$COVERAGE_INITIAL" ]; then
            echo '{"totals": {"percent_covered": 0}, "files": {}}' > "$COVERAGE_INITIAL"
          fi
          
          # Extract coverage safely and trim newlines
          INITIAL_COV=$(jq -r '.totals.percent_covered // 0' "$COVERAGE_INITIAL")
          INITIAL_COV=$(echo "$INITIAL_COV" | tr -d '\n' | xargs)
          echo "initial_coverage=$INITIAL_COV" >> $GITHUB_OUTPUT
          echo "✅ Initial coverage: ${INITIAL_COV}%" | tee -a generation_metrics.txt

      - name: 🔍 Find modules needing tests
        id: find-modules
        run: |
          set -euo pipefail
          
          THRESHOLD=${{ env.VALIDATED_THRESHOLD }}
          echo "🔍 Finding modules below ${THRESHOLD}% coverage..."
          
          # Run gap finder
          if [ -f ".github/scripts/find_coverage_gaps.py" ]; then
            poetry run python .github/scripts/find_coverage_gaps.py "$THRESHOLD" > modules.json || {
              echo "::warning::Gap finder failed, using empty list"
              echo "[]" > modules.json
            }
          else
            echo "::error::find_coverage_gaps.py not found"
            echo "[]" > modules.json
          fi
          
          # Validate JSON
          if ! jq empty modules.json 2>/dev/null; then
            echo "[]" > modules.json
          fi
          
          # Count and output with trimmed newlines
          MODULE_COUNT=$(jq '. | length' modules.json)
          MODULE_COUNT=$(echo "$MODULE_COUNT" | tr -d '\n' | xargs)
          echo "module_count=$MODULE_COUNT" >> $GITHUB_OUTPUT
          echo "modules=$(cat modules.json | tr -d '\n')" >> $GITHUB_OUTPUT
          echo "✅ Found $MODULE_COUNT modules" | tee -a generation_metrics.txt
          
          # Log module list
          if [ "$MODULE_COUNT" -gt 0 ]; then
            echo "Modules:" | tee -a generation_metrics.txt
            jq -r '.[]' modules.json | sed 's/^/  - /' | tee -a generation_metrics.txt
          fi

      - name: 🤖 Generate tests
        id: generate
        run: |
          set -euo pipefail
          
          MODULE_COUNT=${{ steps.find-modules.outputs.module_count }}
          
          if [ "$MODULE_COUNT" -eq "0" ]; then
            echo "ℹ️ No modules need tests"
            echo "status=no_modules" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Verify script exists
          if [ ! -f ".github/scripts/generate_test2.py" ]; then
            echo "::error::generate_test2.py not found"
            echo "status=script_missing" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Process modules
          MODULES='${{ steps.find-modules.outputs.modules }}'
          MAX_MODULES=${{ env.VALIDATED_MAX_MODULES }}
          
          # Apply limit
          if [ "$MAX_MODULES" -gt 0 ] && [ "$MODULE_COUNT" -gt "$MAX_MODULES" ]; then
            MODULES=$(echo "$MODULES" | jq ".[:$MAX_MODULES]")
            echo "ℹ️ Limited to $MAX_MODULES modules" | tee -a generation_metrics.txt
          fi
          
          # Initialize counters
          echo "0" > "${TEMP_DIR}/success_count"
          echo "0" > "${TEMP_DIR}/failed_count"
          START_TIME=$(date +%s)
          
          # Process each module - fix race condition by using array
          SUCCESS_COUNT=0
          FAILED_COUNT=0
          
          while IFS= read -r MODULE; do
            echo "🔄 Processing: $MODULE" | tee -a generation_metrics.txt
            
            if poetry run python .github/scripts/generate_test2.py --module "$MODULE"; then
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              echo "  ✅ Success" | tee -a generation_metrics.txt
            else
              FAILED_COUNT=$((FAILED_COUNT + 1))
              echo "  ❌ Failed" | tee -a generation_metrics.txt
            fi
          done < <(echo "$MODULES" | jq -r '.[]')
          
          # Save final counts
          echo "$SUCCESS_COUNT" > "${TEMP_DIR}/success_count"
          echo "$FAILED_COUNT" > "${TEMP_DIR}/failed_count"
          
          # Read final counts and trim newlines
          SUCCESS=$(cat "${TEMP_DIR}/success_count" 2>/dev/null || echo "0")
          SUCCESS=$(echo "$SUCCESS" | tr -d '\n' | xargs)
          FAILED=$(cat "${TEMP_DIR}/failed_count" 2>/dev/null || echo "0")
          FAILED=$(echo "$FAILED" | tr -d '\n' | xargs)
          DURATION=$(($(date +%s) - START_TIME))
          
          echo "Generation Summary:" | tee -a generation_metrics.txt
          echo "  Duration: ${DURATION}s" | tee -a generation_metrics.txt
          echo "  Success: $SUCCESS" | tee -a generation_metrics.txt
          echo "  Failed: $FAILED" | tee -a generation_metrics.txt
          
          # Count generated tests
          TEST_COUNT=$(find tests -name "test_*.py" -not -name "test_placeholder.py" | wc -l)
          
          if [ "$TEST_COUNT" -gt 0 ]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "test_count=$(echo $TEST_COUNT | tr -d '\n')" >> $GITHUB_OUTPUT
            echo "✅ Generated $TEST_COUNT test files" | tee -a generation_metrics.txt
          else
            echo "status=no_tests" >> $GITHUB_OUTPUT
            echo "test_count=0" >> $GITHUB_OUTPUT
          fi

      - name: 🧪 Run tests
        id: test-run
        run: |
          set -euo pipefail
          
          # Count actual test files
          TEST_COUNT=$(find tests -name "test_*.py" -not -name "test_placeholder.py" | wc -l)
          
          if [ "$TEST_COUNT" -eq 0 ]; then
            echo "ℹ️ No tests to run"
            echo "status=no_tests" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "🧪 Running $TEST_COUNT test files" | tee -a generation_metrics.txt
          
          # Run tests (allow failure)
          set +e
          poetry run pytest tests/ \
            -v \
            --tb=short \
            --cov="${SRC_DIR}" \
            --cov-report="json:${COVERAGE_AFTER_GEN}" \
            2>&1 | tee test-results.log
          
          TEST_EXIT_CODE=$?
          set -e
          
          # Generate final reports
          poetry run pytest tests/ \
            --cov="${SRC_DIR}" \
            --cov-report="html:coverage-final/" \
            --cov-report="json:coverage-final/${COVERAGE_FINAL}" \
            --quiet 2>/dev/null || true
          
          # Analyze results with safe grep and trim newlines
          if [ "$TEST_EXIT_CODE" -ne 0 ]; then
            # Initialize counters
            FAILURE_COUNT=0
            EXPECTED_FAILURES=0
            
            # Get RUN_ID from environment
            RUN_ID="${GITHUB_RUN_ID}_${GITHUB_RUN_NUMBER}"
            
            # Count total failures safely
            if [ -f "test-results.log" ]; then
              # Count FAILED lines
              FAILURE_COUNT=$(grep -c "FAILED" test-results.log 2>/dev/null || echo "0")
              # Count expected failures (pytest.raises)
              EXPECTED_FAILURES=$(grep -c "pytest.raises" test-results.log 2>/dev/null || echo "0")
              
              # Convert to integers safely
              FAILURE_COUNT=$(printf '%d' "${FAILURE_COUNT:-0}" 2>/dev/null || echo "0")
              EXPECTED_FAILURES=$(printf '%d' "${EXPECTED_FAILURES:-0}" 2>/dev/null || echo "0")
              
              # Calculate actual failures
              if [ "$FAILURE_COUNT" -ge "$EXPECTED_FAILURES" ]; then
                ACTUAL_FAILURES=$((FAILURE_COUNT - EXPECTED_FAILURES))
              else
                ACTUAL_FAILURES=0
              fi
            else
              ACTUAL_FAILURES=0
            fi
            
            # Log results
            echo "Test Results Summary (Run ID: $RUN_ID):" | tee -a generation_metrics.txt
            echo "  Total Failures: $FAILURE_COUNT" | tee -a generation_metrics.txt
            echo "  Expected Failures: $EXPECTED_FAILURES" | tee -a generation_metrics.txt
            echo "  Actual Failures: $ACTUAL_FAILURES" | tee -a generation_metrics.txt
            
            # Extract error details for debugging
            if [ -f "test-results.log" ]; then
              echo "Error Details:" | tee -a generation_metrics.txt
              grep -A 5 "FAILED\|ERROR" test-results.log | tee -a generation_metrics.txt
            fi
            
            # Set status based on actual failures
            if [ "$ACTUAL_FAILURES" -gt 0 ]; then
              echo "⚠️ $ACTUAL_FAILURES unexpected test failures" | tee -a generation_metrics.txt
              echo "status=has_failures" >> $GITHUB_OUTPUT
              echo "failure_count=$ACTUAL_FAILURES" >> $GITHUB_OUTPUT
            else
              echo "✅ All unexpected tests passed!" | tee -a generation_metrics.txt
              echo "status=all_passed" >> $GITHUB_OUTPUT
              echo "failure_count=0" >> $GITHUB_OUTPUT
            fi
          else
            echo "✅ All tests passed!" | tee -a generation_metrics.txt
            echo "status=all_passed" >> $GITHUB_OUTPUT
            echo "failure_count=0" >> $GITHUB_OUTPUT
          fi

      - name: 📊 Generate coverage summary
        if: always()
        run: |
          set -euo pipefail
          
          echo "📊 Generating summary..." | tee -a generation_metrics.txt
          
          # Try main script
          if [ -f ".github/scripts/create_summary.py" ]; then
            poetry run python .github/scripts/create_summary.py || {
              echo "::warning::Summary script failed"
              # Fallback inline
              echo "# Coverage Summary" > COVERAGE_SUMMARY.md
              
              if [ -f "$COVERAGE_INITIAL" ]; then
                INITIAL=$(jq -r '.totals.percent_covered // 0' "$COVERAGE_INITIAL")
                echo "**Initial**: ${INITIAL}%" >> COVERAGE_SUMMARY.md
              fi
              
              if [ -f "coverage-final/$COVERAGE_FINAL" ]; then
                FINAL=$(jq -r '.totals.percent_covered // 0' "coverage-final/$COVERAGE_FINAL")
                echo "**Final**: ${FINAL}%" >> COVERAGE_SUMMARY.md
                
                if [ -n "${INITIAL:-}" ]; then
                  # Use safe arithmetic
                  if command -v bc >/dev/null 2>&1; then
                    DELTA=$(echo "scale=1; $FINAL - $INITIAL" | bc)
                  else
                    # Fallback to integer math if bc not available
                    DELTA=$((${FINAL%.*} - ${INITIAL%.*}))
                  fi
                  echo "**Change**: ${DELTA}%" >> COVERAGE_SUMMARY.md
                fi
              fi
            }
          fi

      - name: �� Collect experiment data
        if: always()
        run: |
          set -euo pipefail
          
          echo "📦 Collecting data..." | tee -a generation_metrics.txt
          
          # Try enhanced collector
          if [ -f ".github/scripts/enhanced_collect_experiment_data.py" ]; then
            poetry run python .github/scripts/enhanced_collect_experiment_data.py || {
              echo "::warning::Enhanced collector failed, using fallback"
              
              # Simple fallback
              TIMESTAMP=$(date +%Y%m%d_%H%M%S)
              ARCHIVE_DIR="experiment_data_${TIMESTAMP}"
              mkdir -p "$ARCHIVE_DIR"
              
              # Copy all artifacts
              for pattern in tests "*.json" "*.log" "*.txt" "*.md" coverage-final; do
                if ls $pattern >/dev/null 2>&1; then
                  cp -r $pattern "$ARCHIVE_DIR/" 2>/dev/null || true
                fi
              done
              
              zip -r "${ARCHIVE_DIR}.zip" "$ARCHIVE_DIR" || true
            }
          fi

      - name: 💾 Upload test files as artifact
        uses: actions/upload-artifact@v4
        with:
          name: test-files-${{ github.run_id }}
          path: tests/
          if-no-files-found: error

      - name: 💾 Upload coverage files as artifact
        uses: actions/upload-artifact@v4
        with:
          name: coverage-files-${{ github.run_id }}
          path: |
            coverage-initial.json
            coverage-final.json
            coverage-after-generation.json
          if-no-files-found: error

      - name: 💾 Upload experiment data as artifact
        uses: actions/upload-artifact@v4
        with:
          name: experiment-data-${{ github.run_id }}
          path: |
            experiment_data_*.zip
            *.json
            *.log
            *.txt
            *.md
          if-no-files-found: warn

      - name: 📝 Create workflow summary
        if: always()
        run: |
          cat <<EOF >> $GITHUB_STEP_SUMMARY
          # Test Generation Summary
          
          ## Overview
          - **Initial Coverage**: ${{ steps.initial-coverage.outputs.initial_coverage }}%
          - **Modules Found**: ${{ steps.find-modules.outputs.module_count }}
          - **Tests Generated**: ${{ steps.generate.outputs.test_count }}
          - **Test Status**: ${{ steps.test-run.outputs.status }}
          
          ## Configuration
          - **Coverage Threshold**: ${{ env.VALIDATED_THRESHOLD }}%
          - **Max Modules**: ${{ env.VALIDATED_MAX_MODULES }}
          
          ## Details
          EOF
          
          if [ "${{ steps.test-run.outputs.status }}" = "has_failures" ]; then
            cat <<EOF >> $GITHUB_STEP_SUMMARY
          - **Initial Failures**: ${{ steps.test-run.outputs.failure_count }}
          EOF
          fi
          
          # Add coverage summary
          if [ -f "COVERAGE_SUMMARY.md" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            cat COVERAGE_SUMMARY.md >> $GITHUB_STEP_SUMMARY
          fi
          
          # Add metrics
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Metrics" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          tail -30 generation_metrics.txt >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No metrics"
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: �� Cleanup
        if: always()
        run: |
          # Remove temporary files
          rm -rf "${TEMP_DIR}" 2>/dev/null || true
          
          # Remove placeholder test if real tests exist
          if [ -f "tests/test_placeholder.py" ]; then
            REAL_TESTS=$(find tests -name "test_*.py" -not -name "test_placeholder.py" | wc -l)
            if [ "$REAL_TESTS" -gt 0 ]; then
              rm -f tests/test_placeholder.py
            fi
          fi