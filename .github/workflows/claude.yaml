name: Auto Generate Tests with Complete Data Collection

on:
  push:
  workflow_dispatch:
    inputs:
      coverage_threshold:
        description: 'Minimum coverage percentage to target'
        required: false
        default: '80'
      max_modules:
        description: 'Maximum number of modules to analyze (0 = no limit)'
        required: false
        default: '5'
      collect_data:
        description: 'Collect comprehensive experiment data'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'

env:
  DEFAULT_PYTHON: "3.11"
  OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}

jobs:
  generate-tests:
    name: Generate Tests and Collect Complete Data
    runs-on: ubuntu-latest
    outputs:
      has_generated_tests: ${{ steps.test-generation.outputs.has_generated_tests }}
      has_failed_tests: ${{ steps.test-run.outputs.has_failed_tests }}
    steps:
      - name: ⤵️ Check out code from GitHub
        uses: actions/checkout@v4.2.2
      
      - name: 🏗 Set up Poetry
        run: pipx install poetry
      
      - name: 🏗 Set up Python ${{ env.DEFAULT_PYTHON }}
        id: python
        uses: actions/setup-python@v5.5.0
        with:
          python-version: ${{ env.DEFAULT_PYTHON }}
          cache: "poetry"
      
      - name: 🏗 Install workflow dependencies
        run: |
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true
      
      - name: 🏗 Install project dependencies
        run: poetry install --no-interaction
      
      - name: 🏗 Install AI test generation dependencies
        run: poetry add pytest-coverage openai
      
      # Initialize data collection
      - name: 📋 Initialize data collection
        run: |
          # Create necessary directories
          mkdir -p data_collection/{metrics,logs,artifacts,reports,visualizations}
          
          # Initialize log files
          echo '[]' > api_interaction_logs.json
          echo "Experiment started at $(date)" > generation_metrics.txt
          echo "Repository: ${{ github.repository }}" >> generation_metrics.txt
          echo "Run ID: ${{ github.run_id }}" >> generation_metrics.txt
          echo "Run Number: ${{ github.run_number }}" >> generation_metrics.txt
      
      # Step 1: Initial Coverage Analysis
      - name: 📊 Generate initial coverage report
        run: |
          poetry run pytest --cov=src --cov-report=json:coverage-initial.json || true
          echo "Initial coverage generated at $(date)" >> generation_metrics.txt
      
      # Step 2: Find files needing tests
      - name: 🔍 Find modules needing tests
        id: find-modules
        run: |
          COVERAGE_THRESHOLD=${{ github.event.inputs.coverage_threshold }}
          poetry run python .github/scripts/find_coverage_gaps.py $COVERAGE_THRESHOLD > modules.json
          
          # Check if we found any modules
          MODULE_COUNT=$(cat modules.json | jq '. | length')
          echo "Found $MODULE_COUNT modules below $COVERAGE_THRESHOLD% coverage" >> generation_metrics.txt
          
          if [ "$MODULE_COUNT" -gt 0 ]; then
            echo "has_modules=true" >> $GITHUB_OUTPUT
          else
            echo "has_modules=false" >> $GITHUB_OUTPUT
          fi
          
          echo "modules=$(cat modules.json)" >> $GITHUB_OUTPUT
      
      # Step 3: Generate tests for each module
      - name: 🤖 Generate initial tests
        id: test-generation
        if: steps.find-modules.outputs.has_modules == 'true'
        run: |
          TOTAL_START=$(date +%s)
          
          MODULES='${{ steps.find-modules.outputs.modules }}'
          MAX_MODULES=${{ github.event.inputs.max_modules }}
          
          # Limit modules if specified
          if [ "$MAX_MODULES" != "0" ] && [ "$MAX_MODULES" -gt 0 ]; then
            MODULES=$(echo "$MODULES" | jq --arg max "$MAX_MODULES" '. | .[0:($max | tonumber)]')
          fi
          
          echo "Processing $(echo "$MODULES" | jq '. | length') modules" >> generation_metrics.txt
          
          # Track if we generated any tests
          GENERATED_COUNT=0
          
          echo "$MODULES" | jq -r '.[]' | while read MODULE; do
            echo "=== Generating tests for $MODULE ===" | tee -a generation_metrics.txt
            
            MODULE_START=$(date +%s)
            
            # Run test generation
            if poetry run python .github/scripts/generate_test2.py --module "$MODULE" 2>&1 | tee -a test_generation.log; then
              GENERATED_COUNT=$((GENERATED_COUNT + 1))
            fi
            
            MODULE_END=$(date +%s)
            MODULE_DURATION=$((MODULE_END - MODULE_START))
            
            echo "Module: $MODULE, Time: $MODULE_DURATION seconds" >> generation_metrics.txt
          done
          
          TOTAL_END=$(date +%s)
          TOTAL_DURATION=$((TOTAL_END - TOTAL_START))
          echo "Total generation time: $TOTAL_DURATION seconds" >> generation_metrics.txt
          
          # Check if any tests were generated
          if [ -d "tests" ] && [ "$(find tests -name "test_*.py" | wc -l)" -gt 0 ]; then
            echo "has_generated_tests=true" >> $GITHUB_OUTPUT
          else
            echo "has_generated_tests=false" >> $GITHUB_OUTPUT
          fi
      
      # Step 4: Run generated tests - FIXED CONDITION
      - name: 🧪 Run generated tests with coverage
        id: test-run
        if: steps.test-generation.outputs.has_generated_tests == 'true' || steps.find-modules.outputs.has_modules != 'true'
        run: |
          echo "Running tests at $(date)" >> generation_metrics.txt
          
          # Always create test-errors.log
          touch test-errors.log
          
          # Run with detailed output
          if poetry run pytest tests/ -v --tb=short --cov=src --cov-report=json:coverage-post-generation.json 2>&1 | tee test-results.log; then
            echo "All tests passed" > test-errors.log
            echo "has_failed_tests=false" >> $GITHUB_OUTPUT
          else
            grep -E "(FAILED|ERROR)" test-results.log > test-errors.log || true
            echo "has_failed_tests=true" >> $GITHUB_OUTPUT
          fi
          
          # Create multiple coverage report formats
          poetry run pytest tests/ -v --cov=src --cov-report=html:coverage-final/ || true
          poetry run pytest tests/ -v --cov=src --cov-report=json:coverage-final/coverage.json || true
          poetry run pytest tests/ -v --cov=src --cov-report=term-missing > coverage-terminal.txt || true
      
      # Step 5: Refine failing tests - FIXED CONDITION
      - name: 🔄 Refine failing tests
        if: steps.test-run.outputs.has_failed_tests == 'true'
        run: |
          echo "Starting refinement at $(date)" >> generation_metrics.txt
          REFINE_START=$(date +%s)
          
          poetry run python .github/scripts/refine_tests.py 2>&1 | tee refinement.log || true
          
          REFINE_END=$(date +%s)
          REFINE_DURATION=$((REFINE_END - REFINE_START))
          echo "Refinement time: $REFINE_DURATION seconds" >> generation_metrics.txt
          
          # Run tests again after refinement
          poetry run pytest tests/ -v --cov=src --cov-report=json:coverage-final/coverage.json || true
      
      # Step 6: Generate coverage summary - ALWAYS RUN
      - name: 📊 Generate coverage summary
        if: always()
        run: |
          poetry run python .github/scripts/create_summary.py || echo "Failed to create summary"
          
          # Add summary to metrics
          echo "=== Coverage Summary ===" >> generation_metrics.txt
          cat COVERAGE_SUMMARY.md >> generation_metrics.txt || echo "No summary available" >> generation_metrics.txt
      
      # Step 7: Collect experiment data - USE STRING COMPARISON
      - name: 📦 Collect comprehensive experiment data
        if: github.event.inputs.collect_data == 'true'
        run: |
          echo "Running comprehensive data collection at $(date)" >> generation_metrics.txt
          
          # Run the enhanced data collection script
          if poetry run python .github/scripts/enhanced_collect_experiment_data.py; then
            echo "Data collection completed successfully"
            # List generated files
            echo "Generated data files:"
            ls -la experiment_data_*/ || true
          else
            echo "Data collection failed, but continuing..."
          fi
      
      # Step 8: Upload artifacts - ALWAYS RUN
      - name: 💾 Upload test generation artifacts
        if: always()
        uses: actions/upload-artifact@v4.6.2
        with:
          name: test-generation-artifacts
          path: |
            tests/**/test_*.py
            coverage-*.json
            coverage-final/*
            coverage-terminal.txt
            COVERAGE_SUMMARY.md
            generation_metrics.txt
            test-results.log
            test-errors.log
            test_generation.log
            refinement.log
            api_interaction_logs.json
            refinement_log.json
            modules.json
            !tests/**/__pycache__/**
          retention-days: 30
          if-no-files-found: warn
      
      # Step 9: Upload experiment data - FIXED CONDITION
      - name: 💾 Upload experiment data archive
        if: github.event.inputs.collect_data == 'true'
        uses: actions/upload-artifact@v4.6.2
        with:
          name: experiment-data-complete
          path: experiment_data_complete_*.zip
          retention-days: 90
          if-no-files-found: warn
      
      # Step 10: Create workflow summary - ALWAYS RUN
      - name: 📝 Create detailed workflow summary
        if: always()
        run: |
          echo "# Test Generation Experiment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Metadata
          echo "## Experiment Metadata" >> $GITHUB_STEP_SUMMARY
          echo "- **Time**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "- **Repository**: ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Run ID**: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Coverage Threshold**: ${{ github.event.inputs.coverage_threshold }}%" >> $GITHUB_STEP_SUMMARY
          echo "- **Data Collection**: ${{ github.event.inputs.collect_data }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Coverage summary
          if [ -f "COVERAGE_SUMMARY.md" ]; then
            cat COVERAGE_SUMMARY.md >> $GITHUB_STEP_SUMMARY
          else
            echo "No coverage summary available" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Test statistics
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Generation Statistics" >> $GITHUB_STEP_SUMMARY
          
          if [ -d "tests" ]; then
            TEST_FILES=$(find tests -name "test_*.py" | wc -l)
            echo "- **Test Files Generated**: $TEST_FILES" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Test Files Generated**: 0" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Status
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Status" >> $GITHUB_STEP_SUMMARY
          echo "- **Generated Tests**: ${{ steps.test-generation.outputs.has_generated_tests || 'false' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed Tests**: ${{ steps.test-run.outputs.has_failed_tests || 'false' }}" >> $GITHUB_STEP_SUMMARY
      
      # Step 11: Create PR - ONLY IF SUCCESS
      - name: 🔄 Create Pull Request
        if: success() && (steps.test-generation.outputs.has_generated_tests == 'true')
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "✅ Add AI-generated tests [Run #${{ github.run_number }}]"
          title: "✅ AI-Generated Tests - Coverage Improvement [Run #${{ github.run_number }}]"
          body: |
            ## AI-Generated Test Coverage Improvement
            
            ### Summary
            $(cat COVERAGE_SUMMARY.md || echo "No summary available")
            
            ### Details
            - **Run ID**: ${{ github.run_id }}
            - **Coverage Threshold**: ${{ github.event.inputs.coverage_threshold }}%
            - **Data Collection**: ${{ github.event.inputs.collect_data }}
            
            📎 [View Workflow](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          branch: ai-generated-tests-${{ github.run_id }}
          labels: automated, tests, enhancement

  # Publish data job - FIXED CONDITIONS
  publish-data:
    name: Publish Experiment Data
    needs: generate-tests
    runs-on: ubuntu-latest
    if: github.event.inputs.collect_data == 'true' && always()
    steps:
      - name: ⬇️ Download experiment data
        uses: actions/download-artifact@v4.2.1
        with:
          name: experiment-data-complete
          path: ./
        continue-on-error: true
      
      - name: 🔍 Check downloaded files
        run: |
          echo "Downloaded files:"
          ls -la
          
      - name: 🏷️ Create Release with Data
        if: ${{ hashFiles('experiment_data_complete_*.zip') != '' }}
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: experiment-${{ github.run_number }}
          release_name: Test Generation Experiment - Run ${{ github.run_number }}
          body: |
            ## Test Generation Experiment Results
            
            Complete experiment data archive attached.
            
            📎 [View Workflow](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          draft: false
          prerelease: false