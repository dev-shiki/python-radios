---
name: Auto Generate Tests 3

# You can run this workflow manually from the Actions tab or schedule it
on:
  workflow_dispatch:
    inputs:
      coverage_threshold:
        description: 'Minimum coverage percentage to target'
        required: false
        default: '80'
      max_modules:
        description: 'Maximum number of modules to analyze (0 = no limit)'
        required: false
        default: '5'
  
  # Optionally run on schedule (e.g., weekly)
  # schedule:
  #   - cron: '0 0 * * 0'  # Runs at midnight on Sunday

env:
  DEFAULT_PYTHON: "3.11"
  OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}

jobs:
  find-and-generate:
    name: Generate Tests with Iterative Improvement
    runs-on: ubuntu-latest
    steps:
      - name: â¤µï¸ Check out code from GitHub
        uses: actions/checkout@v4.2.2
      
      - name: ðŸ— Set up Poetry
        run: pipx install poetry
      
      - name: ðŸ— Set up Python ${{ env.DEFAULT_PYTHON }}
        id: python
        uses: actions/setup-python@v5.5.0
        with:
          python-version: ${{ env.DEFAULT_PYTHON }}
          cache: "poetry"
      
      - name: ðŸ— Install workflow dependencies
        run: |
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true
      
      - name: ðŸ— Install project dependencies
        run: poetry install --no-interaction
      
      - name: ðŸ— Install AI test generation dependencies
        run: poetry add pytest-coverage openai
      
      # Step 1: Initial Coverage Analysis
      - name: ðŸ“Š Generate initial coverage report
        run: |
          poetry run pytest --cov=src --cov-report=json:coverage-initial.json || true
          echo "Initial coverage saved to coverage-initial.json"
      
      # Step 2: Find files needing tests
      - name: ðŸ” Find modules needing tests
        id: find-modules
        run: |
          cat > find_coverage_gaps.py << 'EOF'
          import json
          import sys
          
          def find_low_coverage(threshold=80):
              try:
                  with open('coverage-initial.json', 'r') as f:
                      coverage_data = json.load(f)
              except:
                  print("[]")
                  return
              
              low_coverage_files = []
              for file_path, file_data in coverage_data.get('files', {}).items():
                  if '/test_' not in file_path and '/__init__.py' not in file_path:
                      coverage = file_data.get('summary', {}).get('percent_covered', 0)
                      if coverage < threshold:
                          low_coverage_files.append({
                              'path': file_path,
                              'coverage': coverage
                          })
              
              low_coverage_files.sort(key=lambda x: x['coverage'])
              print(json.dumps([f['path'] for f in low_coverage_files]))
          
          if __name__ == "__main__":
              threshold = float(sys.argv[1]) if len(sys.argv) > 1 else 80
              find_low_coverage(threshold)
          EOF
          
          COVERAGE_THRESHOLD=${{ github.event.inputs.coverage_threshold }}
          poetry run python find_coverage_gaps.py $COVERAGE_THRESHOLD > modules.json
          echo "modules=$(cat modules.json)" >> $GITHUB_OUTPUT
      
      # Step 3: Generate tests for each module
      - name: ðŸ¤– Generate initial tests
        if: ${{ fromJson(steps.find-modules.outputs.modules)[0] != null }}
        run: |
          MODULES='${{ steps.find-modules.outputs.modules }}'
          echo "$MODULES" | jq -r '.[]' | while read MODULE; do
            echo "Generating tests for $MODULE"
            poetry run python .github/scripts/generate_test2.py --module "$MODULE"
          done
      
      # Step 4: Run generated tests to get feedback
      - name: ðŸ§ª Run generated tests with coverage
        if: ${{ fromJson(steps.find-modules.outputs.modules)[0] != null }}
        run: |
          poetry run pytest tests/ -v --cov=src --cov-report=json:coverage-post-generation.json 2>&1 | tee test-errors.log
      
      # Step 5: Analyze coverage improvement and test failures
      - name: ðŸ“Š Analyze coverage and test results
        id: analyze-results
        run: |
          cat > analyze_test_results.py << 'EOF'
          import json
          import os
          
          def analyze_coverage():
              initial_data = json.load(open('coverage-initial.json'))
              try:
                  post_data = json.load(open('coverage-post-generation.json'))
              except:
                  print("No post-generation coverage data")
                  return
              
              analysis = {
                  "improvements": [],
                  "still_low": [],
                  "failed_tests": []
              }
              
              # Compare coverage
              for file, file_data in initial_data.get('files', {}).items():
                  initial_coverage = file_data.get('summary', {}).get('percent_covered', 0)
                  post_coverage = post_data.get('files', {}).get(file, {}).get('summary', {}).get('percent_covered', 0)
                  
                  if post_coverage > initial_coverage:
                      analysis["improvements"].append({
                          "file": file,
                          "from": initial_coverage,
                          "to": post_coverage
                      })
                  
                  if post_coverage < 80:
                      analysis["still_low"].append({
                          "file": file,
                          "coverage": post_coverage
                      })
              
              # Check for test errors
              if os.path.exists('test-errors.log'):
                  with open('test-errors.log', 'r') as f:
                      errors = f.read()
                  if 'FAILED' in errors:
                      analysis["failed_tests"] = errors.split('\n')[:20]  # First 20 lines of errors
              
              with open('coverage-analysis.json', 'w') as f:
                  json.dump(analysis, f, indent=2)
              
              # Output for GitHub Actions
              needs_refinement = len(analysis["still_low"]) > 0 or len(analysis["failed_tests"]) > 0
              print(f"needs_refinement={needs_refinement}")
          
          if __name__ == "__main__":
              analyze_coverage()
          EOF
          
          poetry run python analyze_test_results.py >> $GITHUB_OUTPUT
      
      # Step 6: Refine tests based on feedback (only if needed)
      - name: ðŸ”„ Refine failing tests
        run: |
          # Run the test refiner directly
          poetry run python .github/scripts/refine_tests.py
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      
      # Step 7: Final validation
      - name: ðŸ” Final test validation
        run: |
          poetry run pytest tests/ -v
      
      - name: ðŸ“Š Generate final coverage report
        run: |
          poetry run pytest --cov=src --cov-report=html:coverage-final
      
      - name: ðŸ“„ Create coverage summary
        run: |
          cat > create_summary.py << 'EOF'
          import json
          
          try:
              initial = json.load(open('coverage-initial.json'))
              final = json.load(open('coverage-final/coverage.json'))
              
              initial_total = initial.get('totals', {}).get('percent_covered', 0)
              final_total = final.get('totals', {}).get('percent_covered', 0)
              
              with open('COVERAGE_SUMMARY.md', 'w') as f:
                  f.write("# Coverage Summary\n\n")
                  f.write(f"**Initial Coverage**: {initial_total:.1f}%\n")
                  f.write(f"**Final Coverage**: {final_total:.1f}%\n")
                  f.write(f"**Improvement**: {final_total - initial_total:.1f}%\n\n")
                  
                  if os.path.exists('coverage-analysis.json'):
                      with open('coverage-analysis.json', 'r') as af:
                          analysis = json.load(af)
                      
                      f.write("## Coverage Improvements by File\n\n")
                      for improvement in analysis.get('improvements', []):
                          f.write(f"- `{improvement['file']}`: {improvement['from']:.1f}% â†’ {improvement['to']:.1f}%\n")
          except Exception as e:
              print(f"Error creating summary: {e}")
          EOF
          
          poetry run python create_summary.py
      
      - name: ðŸ’¾ Store artifacts
        uses: actions/upload-artifact@v4.6.2
        with:
          name: test-generation-results
          path: |
            tests/**/test_*.py
            coverage-*.json
            coverage-final/*
            COVERAGE_SUMMARY.md
            !tests/**/__pycache__/**
  
  create-pr:
    name: Create Pull Request with Generated Tests
    needs: find-and-generate
    runs-on: ubuntu-latest
    steps:
      - name: â¤µï¸ Check out code from GitHub
        uses: actions/checkout@v4.2.2
      
      - name: â¬‡ï¸ Download test artifacts
        uses: actions/download-artifact@v4.2.1
        with:
          name: test-generation-results
          path: test-results
      
      - name: ðŸ“‚ Prepare files for PR
        run: |
          # Copy test files
          mkdir -p tests
          cp -r test-results/tests/* tests/ 2>/dev/null || true
          
          # Copy coverage reports
          mkdir -p coverage-reports
          cp test-results/coverage-*.json coverage-reports/ 2>/dev/null || true
          cp -r test-results/coverage-final coverage-reports/ 2>/dev/null || true
          
          # Copy summary
          cp test-results/COVERAGE_SUMMARY.md . 2>/dev/null || echo "# Coverage Summary\nNo summary available" > COVERAGE_SUMMARY.md
      
      - name: ðŸ”„ Create Pull Request with generated tests
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "âœ… Add AI-generated tests for modules with low coverage"
          title: "âœ… AI-Generated Tests - Coverage Improvement"
          body: |
            ## AI-Generated Test Coverage Improvement
            
            This PR adds AI-generated tests to improve coverage for modules with coverage below ${{ github.event.inputs.coverage_threshold }}%.
            
            $(cat COVERAGE_SUMMARY.md)
            
            ### Generated Files:
            - Test files are in the `/tests` directory
            - Coverage reports are in `/coverage-reports`
            
            ### Review Notes:
            - Tests were generated and validated automatically
            - Some tests may need manual adjustment
            - Check the coverage summary for improvements
            
            The tests were generated using AI and validated with pytest.
          branch: ai-generated-tests-${{ github.run_id }}
          labels: enhancement, automated, tests