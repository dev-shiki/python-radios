name: Auto Generate Tests with Complete Data Collection

on:
  push:
  workflow_dispatch:
    inputs:
      coverage_threshold:
        description: 'Minimum coverage percentage to target'
        required: false
        default: '80'
      max_modules:
        description: 'Maximum number of modules to analyze (0 = no limit)'
        required: false
        default: '5'
      collect_data:
        description: 'Collect comprehensive experiment data'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'

env:
  DEFAULT_PYTHON: "3.11"
  OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}

jobs:
  generate-tests:
    name: Generate Tests and Collect Complete Data
    runs-on: ubuntu-latest
    steps:
      - name: ⤵️ Check out code from GitHub
        uses: actions/checkout@v4.2.2
      
      - name: 🏗 Set up Poetry
        run: pipx install poetry
      
      - name: 🏗 Set up Python ${{ env.DEFAULT_PYTHON }}
        id: python
        uses: actions/setup-python@v5.5.0
        with:
          python-version: ${{ env.DEFAULT_PYTHON }}
          cache: "poetry"
      
      - name: 🏗 Install workflow dependencies
        run: |
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true
      
      - name: 🏗 Install project dependencies
        run: poetry install --no-interaction
      
      - name: 🏗 Install AI test generation dependencies
        run: poetry add pytest-coverage openai
      
      # Initialize data collection
      - name: 📋 Initialize data collection
        run: |
          # Create necessary directories
          mkdir -p data_collection/{metrics,logs,artifacts,reports,visualizations}
          
          # Initialize log files
          echo '[]' > api_interaction_logs.json
          echo "Experiment started at $(date)" > generation_metrics.txt
          echo "Repository: ${{ github.repository }}" >> generation_metrics.txt
          echo "Run ID: ${{ github.run_id }}" >> generation_metrics.txt
          echo "Run Number: ${{ github.run_number }}" >> generation_metrics.txt
      
      # Step 1: Initial Coverage Analysis
      - name: 📊 Generate initial coverage report
        run: |
          poetry run pytest --cov=src --cov-report=json:coverage-initial.json || true
          echo "Initial coverage generated at $(date)" >> generation_metrics.txt
      
      # Step 2: Find files needing tests
      - name: 🔍 Find modules needing tests
        id: find-modules
        run: |
          COVERAGE_THRESHOLD=${{ github.event.inputs.coverage_threshold }}
          poetry run python .github/scripts/find_coverage_gaps.py $COVERAGE_THRESHOLD > modules.json
          echo "modules=$(cat modules.json)" >> $GITHUB_OUTPUT
          echo "Found $(cat modules.json | jq '. | length') modules below $COVERAGE_THRESHOLD% coverage" >> generation_metrics.txt
      
      # Step 3: Generate tests for each module
      - name: 🤖 Generate initial tests
        if: ${{ fromJson(steps.find-modules.outputs.modules)[0] != null }}
        run: |
          TOTAL_START=$(date +%s)
          
          MODULES='${{ steps.find-modules.outputs.modules }}'
          MAX_MODULES=${{ github.event.inputs.max_modules }}
          
          # Limit modules if specified
          if [ $MAX_MODULES -gt 0 ]; then
            MODULES=$(echo "$MODULES" | jq --arg max "$MAX_MODULES" '. | .[0:($max | tonumber)]')
          fi
          
          echo "Processing $(echo "$MODULES" | jq '. | length') modules" >> generation_metrics.txt
          
          echo "$MODULES" | jq -r '.[]' | while read MODULE; do
            echo "=== Generating tests for $MODULE ===" | tee -a generation_metrics.txt
            
            MODULE_START=$(date +%s)
            
            # Run test generation with enhanced logging
            poetry run python .github/scripts/generate_test2.py --module "$MODULE" 2>&1 | tee -a test_generation.log
            
            MODULE_END=$(date +%s)
            MODULE_DURATION=$((MODULE_END - MODULE_START))
            
            echo "Module: $MODULE, Time: $MODULE_DURATION seconds" >> generation_metrics.txt
          done
          
          TOTAL_END=$(date +%s)
          TOTAL_DURATION=$((TOTAL_END - TOTAL_START))
          echo "Total generation time: $TOTAL_DURATION seconds" >> generation_metrics.txt
      
      # Step 4: Run generated tests with coverage
      - name: 🧪 Run generated tests with coverage
        if: ${{ fromJson(steps.find-modules.outputs.modules)[0] != null }}
        run: |
          echo "Running tests at $(date)" >> generation_metrics.txt
          
          # Run with detailed output
          poetry run pytest tests/ -v --tb=short --cov=src --cov-report=json:coverage-post-generation.json 2>&1 | tee test-results.log
          
          # Create multiple coverage report formats
          poetry run pytest tests/ -v --cov=src --cov-report=html:coverage-final/
          poetry run pytest tests/ -v --cov=src --cov-report=json:coverage-final/coverage.json
          poetry run pytest tests/ -v --cov=src --cov-report=term-missing > coverage-terminal.txt
          
          # Check if any tests failed
          if grep -q "FAILED" test-results.log; then
            grep -E "(FAILED|ERROR)" test-results.log > test-errors.log
            echo "has_failed_tests=true" >> $GITHUB_OUTPUT
          else
            echo "has_failed_tests=false" >> $GITHUB_OUTPUT
            echo "All tests passed" > test-errors.log
          fi
        id: test-run
      
      # Step 5: Refine failing tests
      - name: 🔄 Refine failing tests
        if: ${{ steps.test-run.outputs.has_failed_tests == 'true' }}
        run: |
          echo "Starting refinement at $(date)" >> generation_metrics.txt
          REFINE_START=$(date +%s)
          
          poetry run python .github/scripts/refine_tests.py 2>&1 | tee refinement.log
          
          REFINE_END=$(date +%s)
          REFINE_DURATION=$((REFINE_END - REFINE_START))
          echo "Refinement time: $REFINE_DURATION seconds" >> generation_metrics.txt
          
          # Run tests again after refinement
          poetry run pytest tests/ -v --cov=src --cov-report=json:coverage-final/coverage.json
      
      # Step 6: Generate coverage summary
      - name: 📊 Generate coverage summary
        run: |
          poetry run python .github/scripts/create_summary.py
          
          # Add summary to metrics
          echo "=== Coverage Summary ===" >> generation_metrics.txt
          cat COVERAGE_SUMMARY.md >> generation_metrics.txt
      
      # Step 7: Collect comprehensive experiment data
      - name: 📦 Collect comprehensive experiment data
        if: ${{ github.event.inputs.collect_data == 'true' }}
        run: |
          echo "Running comprehensive data collection at $(date)" >> generation_metrics.txt
          
          # Run the enhanced data collection script
          poetry run python .github/scripts/enhanced_collect_experiment_data.py
          
          # List generated files
          echo "Generated data files:"
          ls -la experiment_data_*/
      
      # Step 8: Upload artifacts
      - name: 💾 Upload test generation artifacts
        uses: actions/upload-artifact@v4.6.2
        with:
          name: test-generation-artifacts
          path: |
            tests/**/test_*.py
            coverage-*.json
            coverage-final/*
            coverage-terminal.txt
            COVERAGE_SUMMARY.md
            generation_metrics.txt
            test-results.log
            test-errors.log
            test_generation.log
            refinement.log
            api_interaction_logs.json
            refinement_log.json
            modules.json
            !tests/**/__pycache__/**
          retention-days: 30
      
      # Step 9: Upload complete experiment data
      - name: 💾 Upload experiment data archive
        if: ${{ github.event.inputs.collect_data == 'true' }}
        uses: actions/upload-artifact@v4.6.2
        with:
          name: experiment-data-complete
          path: experiment_data_complete_*.zip
          retention-days: 90
      
      # Step 10: Create detailed workflow summary
      - name: 📝 Create detailed workflow summary
        run: |
          echo "# Test Generation Experiment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Metadata
          echo "## Experiment Metadata" >> $GITHUB_STEP_SUMMARY
          echo "- **Time**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "- **Repository**: ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Run ID**: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Run Number**: ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Coverage Threshold**: ${{ github.event.inputs.coverage_threshold }}%" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Coverage summary
          if [ -f "COVERAGE_SUMMARY.md" ]; then
            cat COVERAGE_SUMMARY.md >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Generation Statistics" >> $GITHUB_STEP_SUMMARY
          
          # Count metrics
          if [ -d "tests" ]; then
            TEST_FILES=$(find tests -name "test_*.py" | wc -l)
            TEST_FUNCTIONS=$(find tests -name "test_*.py" -exec grep -c "def test_" {} \; | awk '{sum+=$1} END {print sum}')
            ASYNC_TESTS=$(find tests -name "test_*.py" -exec grep -c "async def test_" {} \; | awk '{sum+=$1} END {print sum}')
            ASSERTIONS=$(find tests -name "test_*.py" -exec grep -c "assert" {} \; | awk '{sum+=$1} END {print sum}')
            MOCKS=$(find tests -name "test_*.py" -exec grep -c -E "(Mock|AsyncMock)" {} \; | awk '{sum+=$1} END {print sum}')
            
            echo "- **Test Files**: $TEST_FILES" >> $GITHUB_STEP_SUMMARY
            echo "- **Test Functions**: $TEST_FUNCTIONS" >> $GITHUB_STEP_SUMMARY
            echo "- **Async Tests**: $ASYNC_TESTS" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Assertions**: $ASSERTIONS" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Mocks**: $MOCKS" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Performance metrics
          if [ -f "generation_metrics.txt" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Performance Metrics" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            grep -E "(Total generation time|Refinement time)" generation_metrics.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
          
          # Artifacts info
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Test generation artifacts uploaded" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "experiment_data_complete_*.zip" ]; then
            ZIP_SIZE=$(ls -lh experiment_data_complete_*.zip | awk '{print $5}')
            echo "- ✅ Complete experiment data archive created (Size: $ZIP_SIZE)" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Download Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- [View Workflow Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
      
      # Step 11: Create Pull Request
      - name: 🔄 Create Pull Request
        if: success()
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "✅ Add AI-generated tests [Run #${{ github.run_number }}]"
          title: "✅ AI-Generated Tests - Coverage Improvement [Run #${{ github.run_number }}]"
          body: |
            ## AI-Generated Test Coverage Improvement
            
            This PR contains AI-generated tests with comprehensive metrics.
            
            ### Summary
            $(cat COVERAGE_SUMMARY.md)
            
            ### Experiment Details
            - **Run ID**: `${{ github.run_id }}`
            - **Run Number**: `${{ github.run_number }}`
            - **Coverage Threshold**: `${{ github.event.inputs.coverage_threshold }}%`
            - **Max Modules**: `${{ github.event.inputs.max_modules }}`
            - **Data Collection**: `${{ github.event.inputs.collect_data }}`
            
            ### Generated Files
            - Test files in `/tests` directory
            - Coverage reports in artifacts
            - Complete experiment data archive available
            
            ### Review Notes
            - Tests were generated and validated automatically
            - Some tests may need manual adjustment
            - Full experiment data available in workflow artifacts
            
            📎 [View Workflow Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          branch: ai-generated-tests-${{ github.run_id }}
          labels: automated, tests, enhancement, ai-generated
          
  # Optional job to create release with data
  publish-data:
    name: Publish Experiment Data
    needs: generate-tests
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.collect_data == 'true' && success() }}
    steps:
      - name: ⬇️ Download experiment data
        uses: actions/download-artifact@v4.2.1
        with:
          name: experiment-data-complete
          path: ./
      
      - name: 🏷️ Create Release with Data
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: experiment-${{ github.run_number }}
          release_name: Test Generation Experiment - Run ${{ github.run_number }}
          body: |
            ## Test Generation Experiment Results
            
            **Run Details:**
            - Run ID: `${{ github.run_id }}`
            - Time: ${{ github.event.repository.updated_at }}
            - Coverage Threshold: ${{ github.event.inputs.coverage_threshold }}%
            
            ### Data Archive
            Complete experiment data with all metrics, logs, and artifacts.
            
            📎 [View Workflow](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          draft: false
          prerelease: false