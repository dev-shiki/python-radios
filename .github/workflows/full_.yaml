name: Auto Generate Tests with Complete Data Coll

on:
  push:
  workflow_dispatch:
    inputs:
      coverage_threshold:
        description: 'Minimum coverage percentage to target'
        required: false
        default: '80'
      max_modules:
        description: 'Maximum number of modules to analyze (0 = no limit)'
        required: false
        default: '5'
      max_refinement_attempts:
        description: 'Maximum refinement attempts for failing tests'
        required: false
        default: '2'

env:
  DEFAULT_PYTHON: "3.11"
  OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
  # Standardized coverage file names
  COVERAGE_INITIAL: coverage-initial.json
  COVERAGE_FINAL: coverage-final.json
  COVERAGE_AFTER_GEN: coverage-after-generation.json

# Prevent concurrent runs
concurrency:
  group: test-generation-${{ github.ref }}
  cancel-in-progress: false

jobs:
  generate-tests:
    name: Generate Tests and Collect Data
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 hour timeout
    
    steps:
      - name: ⤵️ Check out code from GitHub
        uses: actions/checkout@v4.2.2
      
      - name: 🔐 Validate prerequisites
        run: |
          echo "🔍 Checking prerequisites..."
          
          # Create temp directory for this run
          RUN_ID="${GITHUB_RUN_ID}_${GITHUB_RUN_NUMBER}"
          export TEMP_DIR="/tmp/test_gen_${RUN_ID}"
          mkdir -p "${TEMP_DIR}"
          echo "TEMP_DIR=${TEMP_DIR}" >> $GITHUB_ENV
          
          # Check API key
          if [ -z "$OPENROUTER_API_KEY" ]; then
            echo "::error::OPENROUTER_API_KEY not set in secrets"
            exit 1
          fi
          echo "✅ API key found"
          
          # Get input parameters
          THRESHOLD="${{ github.event.inputs.coverage_threshold }}"
          MAX_MODULES="${{ github.event.inputs.max_modules }}"
          MAX_ATTEMPTS="${{ github.event.inputs.max_refinement_attempts }}"
          
          # Debug raw inputs
          echo "Debug - Raw inputs:"
          echo "  coverage_threshold: '$THRESHOLD'"
          echo "  max_modules: '$MAX_MODULES'"
          echo "  max_refinement_attempts: '$MAX_ATTEMPTS'"
          
          # Clean inputs - remove all non-numeric characters
          THRESHOLD=$(echo "$THRESHOLD" | sed 's/[^0-9]//g')
          MAX_MODULES=$(echo "$MAX_MODULES" | sed 's/[^0-9]//g')
          MAX_ATTEMPTS=$(echo "$MAX_ATTEMPTS" | sed 's/[^0-9]//g')
          
          # Set defaults if empty
          THRESHOLD=${THRESHOLD:-80}
          MAX_MODULES=${MAX_MODULES:-5}
          MAX_ATTEMPTS=${MAX_ATTEMPTS:-2}
          
          echo "Debug - Cleaned inputs:"
          echo "  coverage_threshold: '$THRESHOLD'"
          echo "  max_modules: '$MAX_MODULES'"
          echo "  max_refinement_attempts: '$MAX_ATTEMPTS'"
          
          # Validate coverage threshold (0-100)
          if [ "$THRESHOLD" -lt 0 ] 2>/dev/null || [ "$THRESHOLD" -gt 100 ] 2>/dev/null; then
            echo "::error::Invalid coverage_threshold: must be 0-100 (got $THRESHOLD)"
            exit 1
          fi
          
          # Validate max modules (>= 0)
          if [ "$MAX_MODULES" -lt 0 ] 2>/dev/null; then
            echo "::error::Invalid max_modules: must be >= 0 (got $MAX_MODULES)"
            exit 1
          fi
          
          # Validate max attempts (>= 1)
          if [ "$MAX_ATTEMPTS" -lt 1 ] 2>/dev/null; then
            echo "::error::Invalid max_refinement_attempts: must be >= 1 (got $MAX_ATTEMPTS)"
            exit 1
          fi
          
          # Export validated values
          echo "VALIDATED_THRESHOLD=$THRESHOLD" >> $GITHUB_ENV
          echo "VALIDATED_MAX_MODULES=$MAX_MODULES" >> $GITHUB_ENV
          echo "VALIDATED_MAX_ATTEMPTS=$MAX_ATTEMPTS" >> $GITHUB_ENV
          
          echo "✅ Validated inputs:"
          echo "  Coverage threshold: $THRESHOLD%"
          echo "  Max modules: $MAX_MODULES"
          echo "  Max refinement attempts: $MAX_ATTEMPTS"
          
          # Check source directory
          if [ ! -d "src" ] && [ ! -f "setup.py" ] && [ ! -f "pyproject.toml" ]; then
            echo "::warning::No standard Python project structure found"
          fi
          
          # Create necessary directories
          mkdir -p tests coverage-final .github/scripts
          
          # Initialize log files with headers
          echo '[]' > api_interaction_logs.json
          echo "# Test Generation Experiment Log" > generation_metrics.txt
          echo "Run ID: ${RUN_ID}" >> generation_metrics.txt
          echo "Started: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> generation_metrics.txt
          echo "Coverage Threshold: $THRESHOLD%" >> generation_metrics.txt
          echo "Max Modules: $MAX_MODULES" >> generation_metrics.txt
          echo "Max Refinement Attempts: $MAX_ATTEMPTS" >> generation_metrics.txt
          echo "---" >> generation_metrics.txt
          
          echo "✅ Prerequisites validated"
      
      - name: 🏗 Set up Poetry
        run: |
          pipx install poetry
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true
      
      - name: 🏗 Set up Python ${{ env.DEFAULT_PYTHON }}
        uses: actions/setup-python@v5.5.0
        with:
          python-version: ${{ env.DEFAULT_PYTHON }}
          cache: "poetry"
      
      - name: 🏗 Install dependencies
        run: |
          set -euo pipefail
          
          # Install base dependencies
          poetry install --no-interaction || {
            echo "::error::Failed to install base dependencies"
            exit 1
          }
          
          # Ensure required packages are installed
          REQUIRED_PACKAGES="pytest pytest-cov pytest-asyncio openai"
          for package in $REQUIRED_PACKAGES; do
            if ! poetry show "$package" &>/dev/null; then
              echo "📦 Installing $package..."
              poetry add "$package" --group dev || {
                echo "::error::Failed to install $package"
                exit 1
              }
            fi
          done
          
          # Verify environment
          poetry check || {
            echo "::warning::Poetry environment check failed"
          }
          
          echo "✅ Dependencies ready"
      
      # Step 1: Initial Coverage
      - name: 📊 Generate initial coverage
        id: initial-coverage
        run: |
          set -euo pipefail
          
          echo "📊 Generating initial coverage..."
          
          # Setup test directory
          touch tests/__init__.py
          
          # Handle no tests case
          if [ -z "$(find tests -name 'test_*.py' -not -name 'test_placeholder.py' 2>/dev/null || true)" ]; then
            echo "def test_placeholder(): pass" > tests/test_placeholder.py
            echo "::warning::No tests found, using placeholder"
          fi
          
          # Determine source directory
          if [ -d "src" ]; then
            SRC_DIR="src"
          else
            SRC_DIR="."
          fi
          echo "SRC_DIR=$SRC_DIR" >> $GITHUB_ENV
          
          # Generate coverage
          poetry run pytest \
            --cov="$SRC_DIR" \
            --cov-report="json:${COVERAGE_INITIAL}" \
            --quiet \
            || true
          
          # Ensure valid coverage file
          if [ ! -f "$COVERAGE_INITIAL" ] || [ ! -s "$COVERAGE_INITIAL" ]; then
            echo '{"totals": {"percent_covered": 0}, "files": {}}' > "$COVERAGE_INITIAL"
          fi
          
          # Extract coverage
          INITIAL_COV=$(jq -r '.totals.percent_covered // 0' "$COVERAGE_INITIAL")
          echo "initial_coverage=$INITIAL_COV" >> $GITHUB_OUTPUT
          echo "✅ Initial coverage: ${INITIAL_COV}%" | tee -a generation_metrics.txt
      
      # Step 2: Find modules needing tests
      - name: 🔍 Find modules needing tests
        id: find-modules
        run: |
          set -euo pipefail
          
          THRESHOLD=${{ env.VALIDATED_THRESHOLD }}
          echo "🔍 Finding modules below ${THRESHOLD}% coverage..."
          
          # Run gap finder
          if [ -f ".github/scripts/find_coverage_gaps.py" ]; then
            poetry run python .github/scripts/find_coverage_gaps.py "$THRESHOLD" > modules.json || {
              echo "::warning::Gap finder failed, using empty list"
              echo "[]" > modules.json
            }
          else
            echo "::error::find_coverage_gaps.py not found"
            echo "[]" > modules.json
          fi
          
          # Validate JSON
          if ! jq empty modules.json 2>/dev/null; then
            echo "[]" > modules.json
          fi
          
          # Count and output
          MODULE_COUNT=$(jq '. | length' modules.json)
          echo "module_count=$MODULE_COUNT" >> $GITHUB_OUTPUT
          echo "modules=$(cat modules.json)" >> $GITHUB_OUTPUT
          echo "✅ Found $MODULE_COUNT modules" | tee -a generation_metrics.txt
          
          # Log module list
          if [ "$MODULE_COUNT" -gt 0 ]; then
            echo "Modules:" | tee -a generation_metrics.txt
            jq -r '.[]' modules.json | sed 's/^/  - /' | tee -a generation_metrics.txt
          fi
      
      # Step 3: Generate tests
      - name: 🤖 Generate tests
        id: generate
        continue-on-error: true
        run: |
          set -euo pipefail
          
          MODULE_COUNT=${{ steps.find-modules.outputs.module_count }}
          
          if [ "$MODULE_COUNT" -eq "0" ]; then
            echo "ℹ️ No modules need tests"
            echo "status=no_modules" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Verify script exists
          if [ ! -f ".github/scripts/generate_test2.py" ]; then
            echo "::error::generate_test2.py not found"
            echo "status=script_missing" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Process modules
          MODULES='${{ steps.find-modules.outputs.modules }}'
          MAX_MODULES=${{ env.VALIDATED_MAX_MODULES }}
          
          # Apply limit
          if [ "$MAX_MODULES" -gt 0 ] && [ "$MODULE_COUNT" -gt "$MAX_MODULES" ]; then
            MODULES=$(echo "$MODULES" | jq ".[:$MAX_MODULES]")
            echo "ℹ️ Limited to $MAX_MODULES modules" | tee -a generation_metrics.txt
          fi
          
          # Initialize counters
          echo "0" > "${TEMP_DIR}/success_count"
          echo "0" > "${TEMP_DIR}/failed_count"
          START_TIME=$(date +%s)
          
          # Process each module
          echo "$MODULES" | jq -r '.[]' | while IFS= read -r MODULE; do
            echo "🔄 Processing: $MODULE" | tee -a generation_metrics.txt
            
            if poetry run python .github/scripts/generate_test2.py --module "$MODULE"; then
              SUCCESS=$(($(cat "${TEMP_DIR}/success_count") + 1))
              echo "$SUCCESS" > "${TEMP_DIR}/success_count"
              echo "  ✅ Success" | tee -a generation_metrics.txt
            else
              FAILED=$(($(cat "${TEMP_DIR}/failed_count") + 1))
              echo "$FAILED" > "${TEMP_DIR}/failed_count"
              echo "  ❌ Failed" | tee -a generation_metrics.txt
            fi
          done
          
          # Read final counts
          SUCCESS=$(cat "${TEMP_DIR}/success_count")
          FAILED=$(cat "${TEMP_DIR}/failed_count")
          DURATION=$(($(date +%s) - START_TIME))
          
          echo "Generation Summary:" | tee -a generation_metrics.txt
          echo "  Duration: ${DURATION}s" | tee -a generation_metrics.txt
          echo "  Success: $SUCCESS" | tee -a generation_metrics.txt
          echo "  Failed: $FAILED" | tee -a generation_metrics.txt
          
          # Count generated tests
          TEST_COUNT=$(find tests -name "test_*.py" -not -name "test_placeholder.py" | wc -l)
          
          if [ "$TEST_COUNT" -gt 0 ]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "test_count=$TEST_COUNT" >> $GITHUB_OUTPUT
            echo "✅ Generated $TEST_COUNT test files" | tee -a generation_metrics.txt
          else
            echo "status=no_tests" >> $GITHUB_OUTPUT
            echo "test_count=0" >> $GITHUB_OUTPUT
          fi
      
      # Step 4: Run tests
      - name: 🧪 Run tests
        id: test-run
        if: always()
        run: |
          set -euo pipefail
          
          # Count actual test files
          TEST_COUNT=$(find tests -name "test_*.py" -not -name "test_placeholder.py" | wc -l)
          
          if [ "$TEST_COUNT" -eq 0 ]; then
            echo "ℹ️ No tests to run"
            echo "status=no_tests" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "🧪 Running $TEST_COUNT test files" | tee -a generation_metrics.txt
          
          # Run tests (allow failure)
          set +e
          poetry run pytest tests/ \
            -v \
            --tb=short \
            --cov="${SRC_DIR}" \
            --cov-report="json:${COVERAGE_AFTER_GEN}" \
            2>&1 | tee test-results.log
          
          TEST_EXIT_CODE=$?
          set -e
          
          # Generate final reports
          poetry run pytest tests/ \
            --cov="${SRC_DIR}" \
            --cov-report="html:coverage-final/" \
            --cov-report="json:coverage-final/${COVERAGE_FINAL}" \
            --quiet || true
          
          # Analyze results
          if [ "$TEST_EXIT_CODE" -ne 0 ]; then
            FAILURE_COUNT=$(grep -c "FAILED" test-results.log || echo "0")
            echo "⚠️ $FAILURE_COUNT test failures" | tee -a generation_metrics.txt
            
            # Extract error details
            grep -A 5 "FAILED\|ERROR" test-results.log > test-errors.log || true
            
            echo "status=has_failures" >> $GITHUB_OUTPUT
            echo "failure_count=$FAILURE_COUNT" >> $GITHUB_OUTPUT
          else
            echo "✅ All tests passed!" | tee -a generation_metrics.txt
            echo "status=all_passed" >> $GITHUB_OUTPUT
            echo "failure_count=0" >> $GITHUB_OUTPUT
          fi
      
      # Step 5: Refine with retry
      - name: 🔄 Refine failing tests
        id: refine
        if: steps.test-run.outputs.status == 'has_failures'
        run: |
          set -euo pipefail
          
          # Verify script
          if [ ! -f ".github/scripts/refine_tests.py" ]; then
            echo "::error::refine_tests.py not found"
            echo "status=script_missing" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          MAX_ATTEMPTS=${{ env.VALIDATED_MAX_ATTEMPTS }}
          INITIAL_FAILURES=${{ steps.test-run.outputs.failure_count }}
          CURRENT_FAILURES=$INITIAL_FAILURES
          
          echo "🔄 Starting refinement (max $MAX_ATTEMPTS attempts)" | tee -a generation_metrics.txt
          echo "Initial failures: $INITIAL_FAILURES" | tee -a generation_metrics.txt
          
          for ATTEMPT in $(seq 1 "$MAX_ATTEMPTS"); do
            if [ "$CURRENT_FAILURES" -eq 0 ]; then
              break
            fi
            
            echo "" | tee -a generation_metrics.txt
            echo "Refinement attempt $ATTEMPT/$MAX_ATTEMPTS" | tee -a generation_metrics.txt
            ATTEMPT_START=$(date +%s)
            
            # Run refinement
            set +e
            poetry run python .github/scripts/refine_tests.py \
              2>&1 | tee "refinement_attempt_${ATTEMPT}.log"
            REFINE_EXIT=$?
            set -e
            
            if [ "$REFINE_EXIT" -ne 0 ]; then
              echo "::warning::Refinement script exited with code $REFINE_EXIT"
            fi
            
            # Test again
            poetry run pytest tests/ \
              -v \
              --tb=short \
              --cov="${SRC_DIR}" \
              --cov-report="json:coverage-final/${COVERAGE_FINAL}" \
              2>&1 | tee "test-results-attempt-${ATTEMPT}.log" || true
            
            # Count failures
            PREVIOUS_FAILURES=$CURRENT_FAILURES
            CURRENT_FAILURES=$(grep -c "FAILED" "test-results-attempt-${ATTEMPT}.log" || echo "0")
            FIXED=$((PREVIOUS_FAILURES - CURRENT_FAILURES))
            DURATION=$(($(date +%s) - ATTEMPT_START))
            
            echo "Attempt $ATTEMPT results:" | tee -a generation_metrics.txt
            echo "  Duration: ${DURATION}s" | tee -a generation_metrics.txt
            echo "  Fixed: $FIXED" | tee -a generation_metrics.txt
            echo "  Remaining: $CURRENT_FAILURES" | tee -a generation_metrics.txt
            
            # Update error log for next iteration
            if [ "$CURRENT_FAILURES" -gt 0 ] && [ "$ATTEMPT" -lt "$MAX_ATTEMPTS" ]; then
              grep -A 5 "FAILED\|ERROR" "test-results-attempt-${ATTEMPT}.log" > test-errors.log || true
            fi
          done
          
          # Final summary
          TOTAL_FIXED=$((INITIAL_FAILURES - CURRENT_FAILURES))
          echo "" | tee -a generation_metrics.txt
          echo "Refinement complete:" | tee -a generation_metrics.txt
          echo "  Total fixed: $TOTAL_FIXED" | tee -a generation_metrics.txt
          echo "  Remaining: $CURRENT_FAILURES" | tee -a generation_metrics.txt
          
          # Set status
          if [ "$CURRENT_FAILURES" -eq 0 ]; then
            echo "status=all_fixed" >> $GITHUB_OUTPUT
          elif [ "$CURRENT_FAILURES" -lt "$INITIAL_FAILURES" ]; then
            echo "status=partial_fix" >> $GITHUB_OUTPUT
          else
            echo "status=no_improvement" >> $GITHUB_OUTPUT
          fi
          
          echo "final_failures=$CURRENT_FAILURES" >> $GITHUB_OUTPUT
      
      # Step 6: Generate summary
      - name: 📊 Generate coverage summary
        if: always()
        run: |
          set -euo pipefail
          
          echo "📊 Generating summary..." | tee -a generation_metrics.txt
          
          # Try main script
          if [ -f ".github/scripts/create_summary.py" ]; then
            poetry run python .github/scripts/create_summary.py || {
              echo "::warning::Summary script failed"
              # Fallback inline
              echo "# Coverage Summary" > COVERAGE_SUMMARY.md
              
              if [ -f "$COVERAGE_INITIAL" ]; then
                INITIAL=$(jq -r '.totals.percent_covered // 0' "$COVERAGE_INITIAL")
                echo "**Initial**: ${INITIAL}%" >> COVERAGE_SUMMARY.md
              fi
              
              if [ -f "coverage-final/$COVERAGE_FINAL" ]; then
                FINAL=$(jq -r '.totals.percent_covered // 0' "coverage-final/$COVERAGE_FINAL")
                echo "**Final**: ${FINAL}%" >> COVERAGE_SUMMARY.md
                
                if [ -n "${INITIAL:-}" ]; then
                  DELTA=$(echo "scale=1; $FINAL - $INITIAL" | bc)
                  echo "**Change**: ${DELTA}%" >> COVERAGE_SUMMARY.md
                fi
              fi
            }
          fi
      
      # Step 7: Collect data
      - name: 📦 Collect experiment data
        if: always()
        run: |
          set -euo pipefail
          
          echo "📦 Collecting data..." | tee -a generation_metrics.txt
          
          # Try enhanced collector
          if [ -f ".github/scripts/enhanced_collect_experiment_data.py" ]; then
            poetry run python .github/scripts/enhanced_collect_experiment_data.py || {
              echo "::warning::Enhanced collector failed"
              
              # Simple fallback
              TIMESTAMP=$(date +%Y%m%d_%H%M%S)
              ARCHIVE_DIR="experiment_data_${TIMESTAMP}"
              mkdir -p "$ARCHIVE_DIR"
              
              # Copy all artifacts
              for pattern in tests "*.json" "*.log" "*.txt" "*.md" coverage-final; do
                cp -r $pattern "$ARCHIVE_DIR/" 2>/dev/null || true
              done
              
              zip -r "${ARCHIVE_DIR}.zip" "$ARCHIVE_DIR/"
            }
          fi
      
      # Step 8: Final summary
      - name: 📝 Create workflow summary
        if: always()
        run: |
          cat <<EOF >> $GITHUB_STEP_SUMMARY
          # Test Generation Summary
          
          ## Overview
          - **Initial Coverage**: ${{ steps.initial-coverage.outputs.initial_coverage }}%
          - **Modules Found**: ${{ steps.find-modules.outputs.module_count }}
          - **Tests Generated**: ${{ steps.generate.outputs.test_count }}
          - **Test Status**: ${{ steps.test-run.outputs.status }}
          
          ## Configuration
          - **Coverage Threshold**: ${{ env.VALIDATED_THRESHOLD }}%
          - **Max Modules**: ${{ env.VALIDATED_MAX_MODULES }}
          - **Max Refinement Attempts**: ${{ env.VALIDATED_MAX_ATTEMPTS }}
          
          ## Details
          EOF
          
          if [ "${{ steps.test-run.outputs.status }}" = "has_failures" ]; then
            cat <<EOF >> $GITHUB_STEP_SUMMARY
          - **Initial Failures**: ${{ steps.test-run.outputs.failure_count }}
          - **Refinement Status**: ${{ steps.refine.outputs.status }}
          - **Final Failures**: ${{ steps.refine.outputs.final_failures }}
          EOF
          fi
          
          # Add coverage summary
          if [ -f "COVERAGE_SUMMARY.md" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            cat COVERAGE_SUMMARY.md >> $GITHUB_STEP_SUMMARY
          fi
          
          # Add metrics
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Metrics" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          tail -30 generation_metrics.txt >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No metrics"
          echo '```' >> $GITHUB_STEP_SUMMARY
      
      # Step 9: Cleanup
      - name: 🧹 Cleanup
        if: always()
        run: |
          # Remove temporary files
          rm -rf "${TEMP_DIR}" 2>/dev/null || true
          rm -f .success_count .failed_count 2>/dev/null || true
          
          # Remove placeholder test if no real tests exist
          if [ -f "tests/test_placeholder.py" ]; then
            REAL_TESTS=$(find tests -name "test_*.py" -not -name "test_placeholder.py" | wc -l)
            if [ "$REAL_TESTS" -gt 0 ]; then
              rm -f tests/test_placeholder.py
            fi
          fi
      
      # Step 10: Upload artifacts
      - name: 💾 Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4.6.2
        with:
          name: test-generation-${{ github.run_id }}
          path: |
            tests/
            coverage-final/
            *.json
            *.log
            *.txt
            *.md
            experiment_data_*.zip
          if-no-files-found: warn
          retention-days: 30