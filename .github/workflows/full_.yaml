name: Auto Generate Tests with Complete Data

on:
  push:
  workflow_dispatch:
    inputs:
      coverage_threshold:
        description: 'Minimum coverage percentage to target'
        required: false
        default: '80'
      max_modules:
        description: 'Maximum number of modules to analyze (0 = no limit)'
        required: false
        default: '5'
      collect_data:
        description: 'Collect comprehensive experiment data'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'

env:
  DEFAULT_PYTHON: "3.11"
  OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}

jobs:
  generate-tests:
    name: Generate Tests and Collect Complete Data
    runs-on: ubuntu-latest
    steps:
      - name: ⤵️ Check out code from GitHub
        uses: actions/checkout@v4.2.2
      
      - name: 🏗 Set up Poetry
        run: pipx install poetry
      
      - name: 🏗 Set up Python ${{ env.DEFAULT_PYTHON }}
        id: python
        uses: actions/setup-python@v5.5.0
        with:
          python-version: ${{ env.DEFAULT_PYTHON }}
          cache: "poetry"
      
      - name: 🏗 Install workflow dependencies
        run: |
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true
      
      - name: 🏗 Install project dependencies
        run: poetry install --no-interaction
      
      - name: 🏗 Install AI test generation dependencies
        run: poetry add pytest-coverage openai
      
      # Initialize data collection
      - name: 📋 Initialize data collection
        run: |
          mkdir -p data_collection/{metrics,logs,artifacts,reports,visualizations}
          echo '[]' > api_interaction_logs.json
          echo "Experiment started at $(date)" > generation_metrics.txt
          echo "Repository: ${{ github.repository }}" >> generation_metrics.txt
          echo "Run ID: ${{ github.run_id }}" >> generation_metrics.txt
      
      # Step 1: Initial Coverage Analysis
      - name: 📊 Generate initial coverage report
        run: |
          poetry run pytest --cov=src --cov-report=json:coverage-initial.json || true
          echo "Initial coverage generated at $(date)" >> generation_metrics.txt
      
      # Step 2: Find files needing tests
      - name: 🔍 Find modules needing tests
        id: find-modules
        run: |
          COVERAGE_THRESHOLD=${{ github.event.inputs.coverage_threshold }}
          poetry run python .github/scripts/find_coverage_gaps.py $COVERAGE_THRESHOLD > modules.json || echo "[]" > modules.json
          echo "modules=$(cat modules.json)" >> $GITHUB_OUTPUT
          echo "Found $(cat modules.json | jq '. | length') modules" >> generation_metrics.txt
      
      # Step 3: Generate tests (dengan continue-on-error)
      - name: 🤖 Generate initial tests
        id: test-generation
        continue-on-error: true  # Lanjut meski ada error
        run: |
          MODULES='${{ steps.find-modules.outputs.modules }}'
          
          # Check if we have modules to process
          if [ "$(echo "$MODULES" | jq '. | length')" -eq 0 ]; then
            echo "No modules to process"
            echo "generation_status=no_modules" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          TOTAL_START=$(date +%s)
          SUCCESS_COUNT=0
          FAIL_COUNT=0
          
          echo "$MODULES" | jq -r '.[]' | while read MODULE; do
            echo "=== Generating tests for $MODULE ===" | tee -a generation_metrics.txt
            MODULE_START=$(date +%s)
            
            if poetry run python .github/scripts/generate_test2.py --module "$MODULE" 2>&1 | tee -a test_generation.log; then
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
            else
              FAIL_COUNT=$((FAIL_COUNT + 1))
              echo "Failed to generate tests for $MODULE" >> generation_metrics.txt
            fi
            
            MODULE_END=$(date +%s)
            echo "Module: $MODULE, Time: $((MODULE_END - MODULE_START)) seconds" >> generation_metrics.txt
          done
          
          TOTAL_END=$(date +%s)
          echo "Total generation time: $((TOTAL_END - TOTAL_START)) seconds" >> generation_metrics.txt
          echo "Success: $SUCCESS_COUNT, Failed: $FAIL_COUNT" >> generation_metrics.txt
          
          # Set status output
          if [ $SUCCESS_COUNT -gt 0 ]; then
            echo "generation_status=partial_success" >> $GITHUB_OUTPUT
          else
            echo "generation_status=all_failed" >> $GITHUB_OUTPUT
          fi
      
      # Step 4: Run tests if any were generated
      - name: 🧪 Run generated tests with coverage
        id: test-run
        if: steps.test-generation.outputs.generation_status != 'all_failed'
        continue-on-error: true
        run: |
          echo "Running tests at $(date)" >> generation_metrics.txt
          
          # Check if any test files exist
          if [ -z "$(find tests -name "test_*.py" -type f 2>/dev/null)" ]; then
            echo "No test files found to run"
            echo "has_tests=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "has_tests=true" >> $GITHUB_OUTPUT
          
          # Run tests
          poetry run pytest tests/ -v --tb=short --cov=src --cov-report=json:coverage-post-generation.json 2>&1 | tee test-results.log || true
          
          # Create coverage reports
          poetry run pytest tests/ -v --cov=src --cov-report=html:coverage-final/ || true
          poetry run pytest tests/ -v --cov=src --cov-report=json:coverage-final/coverage.json || true
          
          # Check for failures
          if grep -q "FAILED" test-results.log; then
            grep -E "(FAILED|ERROR)" test-results.log > test-errors.log
            echo "has_failed_tests=true" >> $GITHUB_OUTPUT
          else
            echo "has_failed_tests=false" >> $GITHUB_OUTPUT
          fi
      
      # Step 5: Refine failing tests
      - name: 🔄 Refine failing tests
        if: steps.test-run.outputs.has_failed_tests == 'true'
        continue-on-error: true
        run: |
          echo "Starting refinement at $(date)" >> generation_metrics.txt
          REFINE_START=$(date +%s)
          
          poetry run python .github/scripts/refine_tests.py 2>&1 | tee refinement.log || {
            echo "Refinement failed" >> generation_metrics.txt
          }
          
          REFINE_END=$(date +%s)
          echo "Refinement time: $((REFINE_END - REFINE_START)) seconds" >> generation_metrics.txt
          
          # Run tests again after refinement
          poetry run pytest tests/ -v --cov=src --cov-report=json:coverage-final/coverage.json || true
      
      # Step 6: Generate coverage summary
      - name: 📊 Generate coverage summary
        continue-on-error: true
        run: |
          poetry run python .github/scripts/create_summary.py || {
            echo "# Coverage Summary" > COVERAGE_SUMMARY.md
            echo "Error generating summary" >> COVERAGE_SUMMARY.md
          }
      
      # Step 7: Collect comprehensive experiment data (ALWAYS)
      - name: 📦 Collect comprehensive experiment data
        if: github.event.inputs.collect_data == 'true' || true  # Always run for now
        continue-on-error: true
        run: |
          echo "Running comprehensive data collection at $(date)" >> generation_metrics.txt
          
          poetry run python .github/scripts/enhanced_collect_experiment_data.py || {
            echo "Data collection failed, creating minimal archive..."
            
            # Create minimal archive if script fails
            mkdir -p experiment_data_minimal
            cp -r coverage-*.json experiment_data_minimal/ 2>/dev/null || true
            cp -r tests experiment_data_minimal/ 2>/dev/null || true
            cp generation_metrics.txt experiment_data_minimal/ 2>/dev/null || true
            
            zip -r experiment_data_minimal_$(date +%Y%m%d_%H%M%S).zip experiment_data_minimal/
          }
          
          # List files
          echo "Data files created:"
          ls -la experiment_data_* || echo "No data files found"
      
      # Step 8: Upload all artifacts
      - name: 💾 Upload test generation artifacts
        uses: actions/upload-artifact@v4.6.2
        with:
          name: test-generation-artifacts
          path: |
            tests/**/test_*.py
            coverage-*.json
            coverage-final/*
            COVERAGE_SUMMARY.md
            generation_metrics.txt
            test-results.log
            test-errors.log
            test_generation.log
            refinement.log
            api_interaction_logs.json
            modules.json
            !tests/**/__pycache__/**
          if-no-files-found: warn
      
      # Step 9: Upload experiment data
      - name: 💾 Upload experiment data archive
        if: always()  # Always try to upload if exists
        uses: actions/upload-artifact@v4.6.2
        with:
          name: experiment-data-complete
          path: |
            experiment_data_complete_*.zip
            experiment_data_minimal_*.zip
          if-no-files-found: warn
      
      # Step 10: Create workflow summary
      - name: 📝 Create workflow summary
        if: always()
        run: |
          echo "# Test Generation Experiment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Status" >> $GITHUB_STEP_SUMMARY
          echo "- Generation Status: ${{ steps.test-generation.outputs.generation_status || 'unknown' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Has Tests: ${{ steps.test-run.outputs.has_tests || 'false' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Has Failed Tests: ${{ steps.test-run.outputs.has_failed_tests || 'unknown' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "COVERAGE_SUMMARY.md" ]; then
            cat COVERAGE_SUMMARY.md >> $GITHUB_STEP_SUMMARY
          else
            echo "## Coverage Summary" >> $GITHUB_STEP_SUMMARY
            echo "No coverage summary available" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Metrics" >> $GITHUB_STEP_SUMMARY
          
          if [ -d "tests" ]; then
            TEST_FILES=$(find tests -name "test_*.py" 2>/dev/null | wc -l || echo 0)
            echo "- Test Files: $TEST_FILES" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f "generation_metrics.txt" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Generation Metrics" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -5 generation_metrics.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
      
      # Step 11: Create Pull Request
      - name: 🔄 Create Pull Request  
        if: always() && steps.test-generation.outputs.generation_status != 'all_failed'
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "✅ Add AI-generated tests [Run #${{ github.run_number }}]"
          title: "✅ AI-Generated Tests - Run #${{ github.run_number }}"
          body: |
            ## AI-Generated Test Results
            
            - Generation Status: ${{ steps.test-generation.outputs.generation_status }}
            - Tests Created: ${{ steps.test-run.outputs.has_tests }}
            - Failed Tests: ${{ steps.test-run.outputs.has_failed_tests }}
            
            ### Artifacts
            - [View Workflow](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          branch: ai-generated-tests-${{ github.run_id }}
          labels: automated, tests