name: Auto Generate Tests with Complete Data 

on:
  push:
  workflow_dispatch:
    inputs:
      coverage_threshold:
        description: 'Minimum coverage percentage to target'
        required: false
        default: '80'
      max_modules:
        description: 'Maximum number of modules to analyze (0 = no limit)'
        required: false
        default: '5'
      collect_data:
        description: 'Collect comprehensive experiment data'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'

env:
  DEFAULT_PYTHON: "3.11"
  OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}

jobs:
  find-and-generate:
    name: Generate Tests with Complete Data Collection
    runs-on: ubuntu-latest
    steps:
      - name: ⤵️ Check out code from GitHub
        uses: actions/checkout@v4.2.2
      
      - name: 🏗 Set up Poetry
        run: pipx install poetry
      
      - name: 🏗 Set up Python ${{ env.DEFAULT_PYTHON }}
        id: python
        uses: actions/setup-python@v5.5.0
        with:
          python-version: ${{ env.DEFAULT_PYTHON }}
          cache: "poetry"
      
      - name: 🏗 Install workflow dependencies
        run: |
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true
      
      - name: 🏗 Install project dependencies
        run: poetry install --no-interaction
      
      - name: 🏗 Install AI test generation dependencies
        run: |
          poetry add pytest-coverage openai
          pip install pandas matplotlib seaborn  # For data collection
      
      # Setup data collection
      - name: 🔧 Setup data collection
        run: |
          # Create directories
          mkdir -p data_collection/{metrics,logs,artifacts,reports,visualizations}
          
          # Initialize log files  
          echo '[]' > api_interaction_logs.json
          echo "Experiment started at $(date)" > generation_metrics.txt
          echo "Repository: ${{ github.repository }}" >> generation_metrics.txt
          echo "Run ID: ${{ github.run_id }}" >> generation_metrics.txt
      
      # Step 1: Initial Coverage Analysis
      - name: 📊 Generate initial coverage report
        run: |
          poetry run pytest --cov=src --cov-report=json:coverage-initial.json || true
          echo "Initial coverage saved to coverage-initial.json"
      
      # Step 2: Find files needing tests
      - name: 🔍 Find modules needing tests
        id: find-modules
        run: |
          cat > find_coverage_gaps.py << 'EOF'
          import json
          import sys
          
          def find_low_coverage(threshold=80):
              try:
                  with open('coverage-initial.json', 'r') as f:
                      coverage_data = json.load(f)
              except:
                  print("[]")
                  return
              
              low_coverage_files = []
              for file_path, file_data in coverage_data.get('files', {}).items():
                  if '/test_' not in file_path and '/__init__.py' not in file_path:
                      coverage = file_data.get('summary', {}).get('percent_covered', 0)
                      if coverage < threshold:
                          low_coverage_files.append({
                              'path': file_path,
                              'coverage': coverage
                          })
              
              low_coverage_files.sort(key=lambda x: x['coverage'])
              print(json.dumps([f['path'] for f in low_coverage_files]))
          
          if __name__ == "__main__":
              threshold = float(sys.argv[1]) if len(sys.argv) > 1 else 80
              find_low_coverage(threshold)
          EOF
          
          COVERAGE_THRESHOLD=${{ github.event.inputs.coverage_threshold }}
          poetry run python find_coverage_gaps.py $COVERAGE_THRESHOLD > modules.json
          echo "modules=$(cat modules.json)" >> $GITHUB_OUTPUT
      
      # Step 3: Generate tests for each module
      - name: 🤖 Generate initial tests
        continue-on-error: true
        run: |
          MODULES='${{ steps.find-modules.outputs.modules }}'
          
          # Check if we have modules
          if [ "$MODULES" = "[]" ]; then
            echo "No modules found below coverage threshold"
            exit 0
          fi
          
          echo "Starting test generation..." >> generation_metrics.txt
          
          echo "$MODULES" | jq -r '.[]' | while read MODULE; do
            echo "Generating tests for $MODULE"
            START_TIME=$(date +%s)
            
            poetry run python .github/scripts/generate_test2.py --module "$MODULE"
            
            END_TIME=$(date +%s)
            echo "Module: $MODULE, Time: $((END_TIME - START_TIME)) seconds" >> generation_metrics.txt
          done
      
      # Step 4: Run generated tests with coverage
      - name: 🧪 Run generated tests with coverage
        continue-on-error: true
        run: |
          # Pastikan file log dibuat
          touch test-results.log
          
          # Run tests dan simpan output
          poetry run pytest tests/ -v --cov=src --cov-report=json:coverage-post-generation.json 2>&1 | tee test-results.log
          
          # Generate final coverage reports
          poetry run pytest tests/ -v --cov=src --cov-report=html:coverage-final/
          poetry run pytest tests/ -v --cov=src --cov-report=json:coverage-final/coverage.json
          
          # Debug: Show test results summary
          echo "=== Test Results Summary ==="
          cat test-results.log | grep -E "(FAILED|PASSED|ERROR)" || echo "No test results found"
          
          # Check if any tests failed
          if grep -q "FAILED" test-results.log; then
            echo "has_failed_tests=true" >> $GITHUB_OUTPUT
            FAILED_COUNT=$(grep -c "FAILED" test-results.log)
            echo "❌ Found $FAILED_COUNT failed tests - will run refiner"
            grep "FAILED" test-results.log > test-errors.log
          else
            echo "has_failed_tests=false" >> $GITHUB_OUTPUT
            echo "✅ All tests passed - will skip refiner"
          fi
        id: test-run
      
      # Debug step to verify outputs (only for troubleshooting)
      - name: 🔍 Debug test run outputs
        if: ${{ github.event.inputs.debug == 'true' }}
        continue-on-error: true
        run: |
          echo "Test run output - has_failed_tests: ${{ steps.test-run.outputs.has_failed_tests }}"
          echo "Test errors log exists: $(test -f test-errors.log && echo 'yes' || echo 'no')"
          if [ -f test-errors.log ]; then
            echo "Test errors content:"
            head -20 test-errors.log
          fi
      
      # Step 5: Analyze coverage improvement and test failures
      - name: 📊 Analyze coverage and test results
        id: analyze-results
        run: |
          cat > analyze_test_results.py << 'EOF'
          import json
          import os
          
          def analyze_coverage():
              initial_data = json.load(open('coverage-initial.json'))
              try:
                  post_data = json.load(open('coverage-post-generation.json'))
              except:
                  print("No post-generation coverage data")
                  return
              
              analysis = {
                  "improvements": [],
                  "still_low": [],
                  "failed_tests": []
              }
              
              # Compare coverage
              for file, file_data in initial_data.get('files', {}).items():
                  initial_coverage = file_data.get('summary', {}).get('percent_covered', 0)
                  post_coverage = post_data.get('files', {}).get(file, {}).get('summary', {}).get('percent_covered', 0)
                  
                  if post_coverage > initial_coverage:
                      analysis["improvements"].append({
                          "file": file,
                          "from": initial_coverage,
                          "to": post_coverage
                      })
                  
                  if post_coverage < 80:
                      analysis["still_low"].append({
                          "file": file,
                          "coverage": post_coverage
                      })
              
              # Check for test errors
              if os.path.exists('test-errors.log'):
                  with open('test-errors.log', 'r') as f:
                      errors = f.read()
                  if 'FAILED' in errors:
                      analysis["failed_tests"] = errors.split('\n')[:20]
              
              with open('coverage-analysis.json', 'w') as f:
                  json.dump(analysis, f, indent=2)
              
              # Output for GitHub Actions
              needs_refinement = len(analysis["failed_tests"]) > 0
              print(f"needs_refinement={needs_refinement}")
          
          if __name__ == "__main__":
              analyze_coverage()
          EOF
          
          poetry run python analyze_test_results.py >> $GITHUB_OUTPUT
      
      # Step 6: Refine failing tests (ONLY runs if tests failed)
      - name: 🔄 Refine failing tests
        if: ${{ steps.test-run.outputs.has_failed_tests == 'true' }}
        continue-on-error: true
        run: |
          echo "Some tests failed. Running test refiner..."
          
          # Check if refine script exists
          if [ -f .github/scripts/refine_tests.py ]; then
            echo "Found refine_tests.py, running it..."
            poetry run python .github/scripts/refine_tests.py
          else
            echo "WARNING: refine_tests.py not found!"
            echo "Skipping test refinement - implement .github/scripts/refine_tests.py to enable"
          fi
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      
      # Step 7: Final validation
      - name: 🔍 Final test validation
        continue-on-error: true
        run: |
          echo "Running final test validation..."
          poetry run pytest tests/ -v
      
      # Step 8: Generate final coverage report
      - name: 📊 Generate final coverage report
        run: |
          poetry run pytest --cov=src --cov-report=html:coverage-final || true
      
      # Step 9: Create coverage summary
      - name: 📄 Create coverage summary
        run: |
          cat > create_summary.py << 'EOF'
          import json
          import os
          
          try:
              initial = json.load(open('coverage-initial.json'))
              final = json.load(open('coverage-final/coverage.json'))
              
              initial_total = initial.get('totals', {}).get('percent_covered', 0)
              final_total = final.get('totals', {}).get('percent_covered', 0)
              
              with open('COVERAGE_SUMMARY.md', 'w') as f:
                  f.write("# Coverage Summary\n\n")
                  f.write(f"**Initial Coverage**: {initial_total:.1f}%\n")
                  f.write(f"**Final Coverage**: {final_total:.1f}%\n")
                  f.write(f"**Improvement**: {final_total - initial_total:.1f}%\n\n")
                  
                  if os.path.exists('coverage-analysis.json'):
                      with open('coverage-analysis.json', 'r') as af:
                          analysis = json.load(af)
                      
                      f.write("## Coverage Improvements by File\n\n")
                      for improvement in analysis.get('improvements', []):
                          f.write(f"- `{improvement['file']}`: {improvement['from']:.1f}% → {improvement['to']:.1f}%\n")
                      
                      if analysis.get('failed_tests'):
                          f.write("\n## Failed Tests\n\n")
                          f.write(f"Number of failed tests: {len(analysis['failed_tests'])}\n\n")
                          for test in analysis['failed_tests'][:5]:
                              if test.strip():
                                  f.write(f"- {test}\n")
          except Exception as e:
              print(f"Error creating summary: {e}")
              with open('COVERAGE_SUMMARY.md', 'w') as f:
                  f.write("# Coverage Summary\n\nError generating summary: " + str(e) + "\n")
          EOF
          
          poetry run python create_summary.py
      
      # Step 10: Collect comprehensive experiment data
      - name: 📦 Collect comprehensive experiment data
        if: always()
        continue-on-error: true
        run: |
          echo "Running comprehensive data collection..."
          
          # Skip if explicitly disabled
          if [ "${{ github.event.inputs.collect_data }}" = "false" ]; then
            echo "Data collection disabled by user input"
            exit 0
          fi
          
          # Create timestamp
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          mkdir -p experiment_data_$TIMESTAMP
          
          # Collect all relevant files
          cp -r tests experiment_data_$TIMESTAMP/ 2>/dev/null || true
          cp *.json experiment_data_$TIMESTAMP/ 2>/dev/null || true
          cp *.txt experiment_data_$TIMESTAMP/ 2>/dev/null || true
          cp *.log experiment_data_$TIMESTAMP/ 2>/dev/null || true
          cp *.md experiment_data_$TIMESTAMP/ 2>/dev/null || true
          cp -r coverage-final experiment_data_$TIMESTAMP/ 2>/dev/null || true
          
          # Create archive
          zip -r experiment_data_$TIMESTAMP.zip experiment_data_$TIMESTAMP/
          
          # List all data files
          echo "Data archives created:"
          ls -la experiment_data_*.zip || echo "No data archives found"
      
      # Step 11: Store artifacts
      - name: 💾 Store artifacts
        uses: actions/upload-artifact@v4.6.2
        with:
          name: test-generation-results
          path: |
            tests/**/test_*.py
            coverage-*.json
            coverage-final/*
            COVERAGE_SUMMARY.md
            generation_metrics.txt
            test-results.log
            test-errors.log
            api_interaction_logs.json
            experiment_data_*.zip
            !tests/**/__pycache__/**
  
  create-pr:
    name: Create Pull Request with Generated Tests
    needs: find-and-generate
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: ⤵️ Check out code from GitHub
        uses: actions/checkout@v4.2.2
      
      - name: ⬇️ Download test artifacts
        uses: actions/download-artifact@v4.2.1
        with:
          name: test-generation-results
          path: test-results
      
      - name: 📂 Prepare files for PR
        run: |
          # Copy test files
          mkdir -p tests
          cp -r test-results/tests/* tests/ 2>/dev/null || true
          
          # Copy coverage reports
          mkdir -p coverage-reports
          cp test-results/coverage-*.json coverage-reports/ 2>/dev/null || true
          cp -r test-results/coverage-final coverage-reports/ 2>/dev/null || true
          
          # Copy summary
          cp test-results/COVERAGE_SUMMARY.md . 2>/dev/null || echo "# Coverage Summary\nNo summary available" > COVERAGE_SUMMARY.md
      
      - name: 🔄 Create Pull Request with generated tests
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "✅ Add AI-generated tests for modules with low coverage"
          title: "✅ AI-Generated Tests - Coverage Improvement"
          body: |
            ## AI-Generated Test Coverage Improvement
            
            This PR adds AI-generated tests to improve coverage for modules with coverage below ${{ github.event.inputs.coverage_threshold }}%.
            
            $(cat COVERAGE_SUMMARY.md)
            
            ### Generated Files:
            - Test files are in the `/tests` directory
            - Coverage reports are in `/coverage-reports`
            
            ### Review Notes:
            - Tests were generated and validated automatically
            - Some tests may need manual adjustment
            - Check the coverage summary for improvements
            
            The tests were generated using AI and validated with pytest.
          branch: ai-generated-tests-${{ github.run_id }}
          labels: enhancement, automated, tests